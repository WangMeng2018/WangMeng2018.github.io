<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>结合知识的推荐系统</title>
      <link href="/2020/03/26/%E7%BB%93%E5%90%88%E7%9F%A5%E8%AF%86%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2020/03/26/%E7%BB%93%E5%90%88%E7%9F%A5%E8%AF%86%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="结合知识的推荐系统"><a href="#结合知识的推荐系统" class="headerlink" title="结合知识的推荐系统"></a>结合知识的推荐系统</h1><p><a href="https://www.microsoft.com/en-us/research/people/xingx/" target="_blank" rel="noopener">谢幸</a></p><p>Resources:</p><ul><li><p><a href="https://mp.weixin.qq.com/s/WgnWaP-NXbVWgQb-4YTfhg" target="_blank" rel="noopener">中文解读</a></p></li><li><p><a href="https://github.com/microsoft/recommenders">推荐系统案例代码</a></p></li></ul><h2 id="1-Knowledge-Graph（知识图谱）"><a href="#1-Knowledge-Graph（知识图谱）" class="headerlink" title="1. Knowledge Graph（知识图谱）"></a>1. Knowledge Graph（知识图谱）</h2><h3 id="1-1-Knowledge-Enhanced-Recommendation（知识增强的推荐系统）"><a href="#1-1-Knowledge-Enhanced-Recommendation（知识增强的推荐系统）" class="headerlink" title="1.1 Knowledge Enhanced Recommendation（知识增强的推荐系统）"></a>1.1 Knowledge Enhanced Recommendation（知识增强的推荐系统）</h3><p>​       知识增强的推荐系统可以有效解决推荐中几大问题：</p><ul><li><p>准确性</p></li><li><ul><li>关于items的更多的语义内容</li><li>更深度的用户兴趣表示</li></ul></li><li><p>多样性</p></li><li><ul><li>知识图谱中不同的关系类型</li><li>通过不同的语义路径拓展用户兴趣</li></ul></li><li><p>可解释性</p></li><li><ul><li>将用户兴趣与推荐结果相连接</li><li>改善用户满意度，提升用户信任度</li><li></li></ul></li></ul><h3 id="1-2-Knowledge-Graph-Embedding（知识图谱嵌入表示）"><a href="#1-2-Knowledge-Graph-Embedding（知识图谱嵌入表示）" class="headerlink" title="1.2 Knowledge Graph Embedding（知识图谱嵌入表示）"></a>1.2 Knowledge Graph Embedding（知识图谱嵌入表示）</h3><p>​        知识图谱嵌入表示是为知识图谱中的每一个实体和关系都学习一个低维的向量表示，使他们可以保持结构和语义知识。</p><ol><li>Distance-based Models（基于距离的模型）</li></ol><ol start="2"><li>Matching-based Models（基于匹配的模型）</li></ol><ol start="3"><li>训练方式</li></ol><h2 id="2-Collaborative-Knowledge-Embedding（协同知识嵌入）"><a href="#2-Collaborative-Knowledge-Embedding（协同知识嵌入）" class="headerlink" title="2. Collaborative Knowledge Embedding（协同知识嵌入）"></a>2. Collaborative Knowledge Embedding（协同知识嵌入）</h2><h3 id="2-1-协同知识嵌入"><a href="#2-1-协同知识嵌入" class="headerlink" title="2.1 协同知识嵌入"></a>2.1 协同知识嵌入</h3><ul><li><p>结构知识</p></li><li><ul><li>Direct, act等</li></ul></li><li><p>视觉知识</p></li><li><ul><li>电影海报，书籍封面图像等</li></ul></li><li><p>文本知识</p></li><li><ul><li>电影描述，评论等</li></ul></li></ul><h3 id="2-2-经典论文"><a href="#2-2-经典论文" class="headerlink" title="2.2 经典论文"></a>2.2 经典论文</h3><ol><li><p>论文名称：Collaborative Knowledge Base Embedding for Recommender Systems, KDD 2016</p></li><li><p>模型：</p></li></ol><ol start="3"><li>数据：</li></ol><h2 id="3-Deep-Knowledge-aware-Network"><a href="#3-Deep-Knowledge-aware-Network" class="headerlink" title="3. Deep Knowledge-aware Network"></a>3. Deep Knowledge-aware Network</h2><h3 id="3-1-经典论文"><a href="#3-1-经典论文" class="headerlink" title="3.1 经典论文"></a>3.1 经典论文</h3><ol><li><p>论文名称：Deep Knowledge-Aware Network for News Recommendation, WWW 2018</p></li><li><p>思路示例图：</p></li></ol><ol start="3"><li><p>处理过程：</p></li><li><p>提取知识表示：</p></li></ol><ol start="5"><li>模型：</li></ol><h2 id="4-Ripple-Network"><a href="#4-Ripple-Network" class="headerlink" title="4. Ripple Network"></a>4. Ripple Network</h2><h3 id="4-1-经典论文"><a href="#4-1-经典论文" class="headerlink" title="4.1 经典论文"></a>4.1 经典论文</h3><ol><li><p>论文名称：Ripple Network: Propagating User Preferences on the Knowledge Graph for Recommender Systems, CIKM 2018</p></li><li><p>思路示例图：</p></li></ol><ol start="3"><li>模型：</li></ol><ol start="4"><li>结果：</li></ol><h2 id="5-可解释性推荐"><a href="#5-可解释性推荐" class="headerlink" title="5. 可解释性推荐"></a>5. 可解释性推荐</h2><h3 id="5-1-经典论文"><a href="#5-1-经典论文" class="headerlink" title="5.1 经典论文"></a>5.1 经典论文</h3><ol><li><p>论文名称：Explainable Recommendation Through Attentive Multi-View Learning</p></li><li><p>模型：</p></li></ol><ol start="3"><li>数据：</li></ol><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><ol><li><p>总结：知识图扩展了每个item的信息量，加强了它们之间的联系；带来推荐结果额外的多样性和可解释性；</p></li><li><p>未来工作：</p><p>•将图形推理与推荐系统集成；</p><p>•联合设计和优化推荐算法和底层架构；</p><p>•处理知识的时间演变；</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning in Materials Science</title>
      <link href="/2020/03/26/Machine-Learning-in-Materials-Science/"/>
      <url>/2020/03/26/Machine-Learning-in-Materials-Science/</url>
      
        <content type="html"><![CDATA[<h1 id="Machine-Learning-in-Materials-Science"><a href="#Machine-Learning-in-Materials-Science" class="headerlink" title="Machine Learning in Materials Science"></a>Machine Learning in Materials Science</h1><p>Resources:</p><ol><li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/inf2.12028" target="_blank" rel="noopener">https://onlinelibrary.wiley.com/doi/full/10.1002/inf2.12028</a></li></ol><h2 id="1-新材料开发"><a href="#1-新材料开发" class="headerlink" title="1. 新材料开发"></a>1. 新材料开发</h2><ol><li><p>计算机模拟材料分子特性</p><p>替代人工挑选，ML加速研发，准确率高，而且专业素质要求低。</p></li><li><p>如何合成</p></li><li><p>优化周期</p></li><li><p>实际应用</p><p>性质、制备、优化、器件结构优化；</p><p>DFT计算材料性质，成本高。</p></li></ol><h2 id="2-作用"><a href="#2-作用" class="headerlink" title="2. 作用"></a>2. 作用</h2><ol><li><p>性质预测</p></li><li><p>宏观材料：图像分析铁轨的腐蚀程度；</p></li><li><p>纳米材料</p></li><li><p>分子性质：字符串分析 + 图像分析</p></li><li><p>增强化学理论</p></li></ol><h2 id="3-新化合物发现"><a href="#3-新化合物发现" class="headerlink" title="3. 新化合物发现"></a>3. 新化合物发现</h2><ol><li><p>结构导向</p></li><li><p>元素导向</p></li><li><p>反向设计</p></li></ol><h2 id="4-展望"><a href="#4-展望" class="headerlink" title="4. 展望"></a>4. 展望</h2><ol><li><p>适用更小的数据集</p></li><li><p>提供更可靠的数据</p></li><li><p>优化材料特征表示</p></li><li><p>增强模型的可解释性</p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>创新在于设计，不在实验；</li><li>文献提取，结构化提取事实关系：The Open Science monitor；</li><li>机器阅读，更可靠地提取结构化内容。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Learning by Semantic Similarity Makes Abstractive Summarization Better</title>
      <link href="/2020/03/11/Report-Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better/"/>
      <url>/2020/03/11/Report-Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better"><a href="#Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better" class="headerlink" title="Learning by Semantic Similarity Makes Abstractive Summarization Better"></a>Learning by Semantic Similarity Makes Abstractive Summarization Better</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        One of the obstacles of abstractive summarization is the presence of various potentially correct predictions.</p><p>​        <strong><em>Semantic Similarity strategy</em></strong>:  consider semantic meanings of generated summaries while training. Our training objective includes maximizing semantic similarity score which is calculated by an additional layer that estimates semantic similarity between generated summary and reference summary.</p><p>​        Code: <a href="https://github.com/icml-2020-nlp/semsim">https://github.com/icml-2020-nlp/semsim</a></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><img src="/2020/03/11/Report-Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better/1.png" title="avatar"><img src="/2020/03/11/Report-Learning-by-Semantic-Similarity-Makes-Abstractive-Summarization-Better/2.png" title="avatar"><p>​        文章认为原来的最大似然目标函数不能处理多个参考摘要的问题，反而会引入噪声，所以使用生成摘要和参考摘要的语义相似度作为目标函数。语义相似度的计算方法很简单，但是很难训练，还需要和最大似然求和，作为最终的目标函数才能加快训练。看实验分析，ROUGE指标的确有提升。</p><p>​        总的来说，该论文的创新度不高。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
            <tag> Abstractive Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: GameWikiSum: a Novel Large Multi-Document Summarization Dataset</title>
      <link href="/2020/03/06/Report-GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset/"/>
      <url>/2020/03/06/Report-GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset/</url>
      
        <content type="html"><![CDATA[<h1 id="GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset"><a href="#GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset" class="headerlink" title="GameWikiSum: a Novel Large Multi-Document Summarization Dataset"></a>GameWikiSum: a Novel Large Multi-Document Summarization Dataset</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        GameWikiSum: <a href="https://github.com/Diego999/GameWikiSum">https://github.com/Diego999/GameWikiSum</a></p><p>​        Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages. Both abstractive and extractive models can be trained on it.</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        The dataset contains 14 652 samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia. Wikipedia pages are available in multiple languages, which opens up the possibility for multilingual multi-document summarization.</p><h2 id="2-GameWikiSum"><a href="#2-GameWikiSum" class="headerlink" title="2. GameWikiSum"></a>2. GameWikiSum</h2><h3 id="2-1-Dataset-Creation"><a href="#2-1-Dataset-Creation" class="headerlink" title="2.1. Dataset Creation"></a>2.1. Dataset Creation</h3><p>​        Reviewed aspects in video games include the gameplay, richness, and diversity of dialogues, or the soundtrack. Compared to usual reviews written by users, these are assumed to be of higher-quality and longer.</p><p>​        In order to match games with their respective Wikipedia pages, we use the game title as the query in the Wikipedia search engine and employ a set of heuristic rules.</p><h3 id="2-2-Heuristic-matching"><a href="#2-2-Heuristic-matching" class="headerlink" title="2.2. Heuristic matching"></a>2.2. Heuristic matching</h3><p>​         Crawl approximately 265 000 professional reviews for around 72 000 games and 26 000 Wikipedia gameplay sections. Design some heuristics because there is no automatic mapping between a game to its Wikipedia page. The heuristics are the followings and applied in this order:</p><ol><li><p>Exact title match: titles must match exactly;</p></li><li><p>Removing tags: when a game has the same name than its franchise, its Wikipedia page has a title similar to Game (year video game) or Game (video game);</p></li><li><p>Extension match: sometimes, a sequel or an extension is not listed in Wikipedia. In this case, we map it to the Wikipedia page of the original game.</p></li></ol><p>We only keep games with at least one review and a matching Wikipedia page, containing a gameplay section.</p><h3 id="2-3-Descriptive-Statistics"><a href="#2-3-Descriptive-Statistics" class="headerlink" title="2.3. Descriptive Statistics"></a>2.3. Descriptive Statistics</h3><img src="/2020/03/06/Report-GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset/1.png" title="avatar"><h2 id="3-Experiments-and-Results"><a href="#3-Experiments-and-Results" class="headerlink" title="3. Experiments and Results"></a>3. Experiments and Results</h2><h3 id="3-1-Evaluation-Metric"><a href="#3-1-Evaluation-Metric" class="headerlink" title="3.1. Evaluation Metric"></a>3.1. Evaluation Metric</h3><img src="/2020/03/06/Report-GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset/2.png" title="avatar"><h3 id="3-3-Results"><a href="#3-3-Results" class="headerlink" title="3.3. Results"></a>3.3. Results</h3><img src="/2020/03/06/Report-GameWikiSum-a-Novel-Large-Multi-Document-Summarization-Dataset/3.png" title="avatar">]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
            <tag> Dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Discriminative Adversarial Search for Abstractive Summarization</title>
      <link href="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/"/>
      <url>/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="Discriminative-Adversarial-Search-for-Abstractive-Summarization"><a href="#Discriminative-Adversarial-Search-for-Abstractive-Summarization" class="headerlink" title="Discriminative Adversarial Search for Abstractive Summarization"></a>Discriminative Adversarial Search for Abstractive Summarization</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        We introduce a novel approach for <strong>sequence decoding</strong>, <strong><em>Discriminative Adversarial Search (DAS)</em></strong>, which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics.（一种新的序列解码方法——判别对抗性搜索(DAS)，该方法在不需要外部指标的情况下，具有减轻暴露偏差影响的优点。）</p><p>​        A discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time.（使用一个鉴别器来改进生成器，不同于GANs的是，在训练时不更新生成器的参数，在推理时仅使用鉴别器来驱动序列生成。）</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        A <strong><em>Teacher Forcing</em></strong> strategy is applied during training: ground-truth tokens are sequentially fed into the model to predict the next token. Conversely, at inference time, ground-truth tokens are not available: the model can only have access to its previous outputs. Such mismatch is referenced to as <strong><em>exposure bias</em></strong>: as mistakes accumulate, this can lead to a divergence from the distribution seen at training time, resulting in poor generation outputs.</p><p>​        To tackle exposure bias, Generative Adversarial Networks (GANs) represent a natural alternative to the proposed approaches: rather than learning a speciﬁc metric, the model learns to generate text that a discriminator can not differentiate from human-produced content.</p><p>​        In GANs, the discriminator is used to improve the generator and is dropped at inference time. DAS do not modify the generator parameters at training time, and use the discriminator at inference time to drive the generation towards human-like textual content.</p><p>​        Contributions:</p><ol><li>propose Discriminative Adversarial Search (DAS), a novel sequence decoding approach that allows to alleviate the effects of exposure bias and to optimize on the data distribution itself rather than for external metrics;</li><li>apply DAS to the abstractive summarization task: even a naively discriminated beam – i.e. without the self-retraining procedure, improves over the state-of-the-art for various metrics;</li><li>report further signiﬁcant improvements when applying discriminator retraining.</li></ol><h2 id="3-Datasets"><a href="#3-Datasets" class="headerlink" title="3. Datasets"></a>3. Datasets</h2><p>Two datasets:</p><ul><li><p>CNN/DM</p></li><li><p>TL;DR （<a href="https://zenodo.org/record/1168855）" target="_blank" rel="noopener">https://zenodo.org/record/1168855）</a></p><p>​        Choose this dataset for two main reasons: ﬁrst, its data is relatively outof-domain if compared to the samples in CNN/DM; second, its characteristics are also quite different: compared to CNN/DM, the TL;DR summaries are twice shorter and three times more abstractive.</p></li></ul><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/1.png" title="avatar"><h2 id="4-Discriminative-Adversarial-Search"><a href="#4-Discriminative-Adversarial-Search" class="headerlink" title="4. Discriminative Adversarial Search"></a>4. Discriminative Adversarial Search</h2><p>​        The proposed model is composed of a generator G (described in 4.1) coupled with a sequential discriminator D (described in 4.2): at inference time, for every new token generated by G, the score and the label assigned by D is used to reﬁne the probabilities, within a beam search, to select the top candidate sequences.（模型由一个生成器G和序列判别器D组成：在推断时，对于G生成的每个新token，D分配分数和标记来调整该token在beam搜索中的概率。）</p><h3 id="4-1-Generator"><a href="#4-1-Generator" class="headerlink" title="4.1. Generator"></a>4.1. Generator</h3><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/2.png" title="avatar"><h3 id="4-2-Discriminator"><a href="#4-2-Discriminator" class="headerlink" title="4.2. Discriminator"></a>4.2. Discriminator</h3><p>​        The objective of the discriminator is to label a sequence Y as being human-produced or machine-generated. At each generation step, the discriminator, instead of predicting the next token among the entire vocabulary V , generates a label among two classes.</p><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/3.png" title="avatar"><h3 id="4-3-Discriminative-Beam-Reranking"><a href="#4-3-Discriminative-Beam-Reranking" class="headerlink" title="4.3. Discriminative Beam Reranking"></a>4.3. Discriminative Beam Reranking</h3><p>​        At each generation step y t , the generator assigns a probability to each of the tokens in its vocabulary V . Use Beam Search to maximize the probability.</p><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/4.png" title="avatar"><p>​        We propose a new score <strong><em>$S_{DAS}$</em></strong> to reﬁne the score <strong><em>$S_{gen}$</em></strong> during the beam search w.r.t. the log probability of the discriminator.</p><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/5.png" title="avatar"><h3 id="4-4-Retraining-the-Discriminator"><a href="#4-4-Retraining-the-Discriminator" class="headerlink" title="4.4. Retraining the Discriminator"></a>4.4. Retraining the Discriminator</h3><p>​        The discriminator can be ﬁne-tuned using the outputs which were found improved via the re-ranking.</p><img src="/2020/03/05/Report-Discriminative-Adversarial-Search-for-Abstractive-Summarization/6.png" title="avatar"><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><ol><li><a href="https://github.com/microsoft/unilm#abstractive-summarization--cnn--daily-mail">https://github.com/microsoft/unilm#abstractive-summarization--cnn--daily-mail</a></li><li><a href="https://zenodo.org/record/1168855" target="_blank" rel="noopener">https://zenodo.org/record/1168855</a></li><li><a href="https://tldr.webis.de/" target="_blank" rel="noopener">https://tldr.webis.de/</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Attend to the beginning: A study on using bidirectional attention for extractive summarization</title>
      <link href="/2020/03/04/Report-Attend-to-the-beginning-A-study-on-using-bidirectional-attention-for-extractive-summarization/"/>
      <url>/2020/03/04/Report-Attend-to-the-beginning-A-study-on-using-bidirectional-attention-for-extractive-summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="Attend-to-the-beginning-A-study-on-using-bidirectional-attention-for-extractive-summarization"><a href="#Attend-to-the-beginning-A-study-on-using-bidirectional-attention-for-extractive-summarization" class="headerlink" title="Attend to the beginning: A study on using bidirectional attention for extractive summarization"></a>Attend to the beginning: A study on using bidirectional attention for extractive summarization</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        This work proposes attending to the beginning of a document, to improve the performance of extractive summarization models when applied to forum discussion data.</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        Text summarization has been applied to different natural language domains; news, academic papers, emails, meeting notes, forum discussions, etc..(新闻、学术文章、邮件、会议记录、论坛交流等等，除了新闻，其他领域的摘要生成都不是成熟。)</p><p>​        This work proposes integrating bidirectional attention mechanism in extractive summarization models, to help to attend to early pieces of text (initial comment). The main objective is to beneﬁt from the dependency between the initial comment and the following comments and try to distinguish between important, and irrelevant or superﬁcial replies.</p><p>​        Contributions in this work are threefold:</p><ol><li>First, we introduce integrating bidirectional attention mechanism into extractive summarization models, to help to attend to earlier pieces of text.</li><li>Second, we achieved a new SOTA on the forum discussion dataset through the proposed attending to the beginning mechanism.</li><li>Third, to further verify the transferability of our hypothesis (i.e. attending to the beginning), we perform evaluations to show that attending to earlier sentence in a more generic text, can also beneﬁt summarization models on different domains other than discussions.</li></ol><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h2><p>​        This work employs two extractive summarization datasets.</p><ol><li>The discussion dataset proposed by (Tarnpradab, Liu, and Hua 2017, <a href="https://www.dropbox.com/s/heevii01b1l6s0a/threadDataSet.zip?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/s/heevii01b1l6s0a/threadDataSet.zip?dl=0</a>) . The discussion dataset is extracted from trip advisor forum discussions.</li><li>Microsoft Word(MSW) dataset(未公开) . MSW dataset was used to verify the transferability of our hypothesis to more generic textual domains.</li></ol><h2 id="3-Baselines"><a href="#3-Baselines" class="headerlink" title="3. Baselines"></a>3. Baselines</h2><ol><li>Sumy <a href="https://pypi.org/project/sumy/" target="_blank" rel="noopener">https://pypi.org/project/sumy/</a></li><li>LSA + clustering</li><li>SummaRuNNer <a href="https://github.com/amagooda/SummaRuNNer_coattention">https://github.com/amagooda/SummaRuNNer_coattention</a></li><li>SiATL</li></ol><h2 id="4-Models"><a href="#4-Models" class="headerlink" title="4. Models"></a>4. Models</h2><img src="/2020/03/04/Report-Attend-to-the-beginning-A-study-on-using-bidirectional-attention-for-extractive-summarization/1.png" title="avatar"><h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><p>​        Experimental designs address the following hypotheses:</p><ul><li><p>Hypothesis 1 (H1) : Attending to the beginning of a discussion thread, would help extractive summarization models to select more salient sentences.（关注一个讨论线程的开始，将有助于提取摘要模型选择更突出的句子。）</p></li><li><p>Hypothesis 2 (H2) : Non-auto-regressive models such as SiATL might be more suitable for thread discussion summarization, compared to auto-regressive models such as SummaRuNNer.（与自动回归模型(如SummaRuNNer)相比，非自回归模型(如SiATL)可能更适合用于线程讨论摘要。）</p></li><li><p>Hypothesis 3 (H3) : Adding additional features, such as contextual embeddings (e.g. BERT) and keywords can give summarization models a boost in performance. （添加额外的特性，例如上下文嵌入(例如BERT)和关键字，可以提高总结模型的性能。）</p></li><li><p>Hypothesis 4 (H4) : Attend to the beginning is transferable to different forms of text other than discussion threads.（注意开头可以转移到除讨论线程之外的其他文本形式。）</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Fill in the BLANC: Human-free Quality Estimation of Document Summaries</title>
      <link href="/2020/03/01/Report-Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries/"/>
      <url>/2020/03/01/Report-Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries/</url>
      
        <content type="html"><![CDATA[<h1 id="Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries"><a href="#Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries" class="headerlink" title="Fill in the BLANC: Human-free Quality Estimation of Document Summaries"></a>Fill in the BLANC: Human-free Quality Estimation of Document Summaries</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        BLANC: automatic estimation of document summary quality. ROUGE require human-written reference summaries, BLANC allowing for fully human-free summary quality estimation.</p><ul><li>objective             客观</li><li>reproducible      可复现</li><li>fully automate   全自动</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Estimation methods:</p><ol><li><p>ROUGE family of methods: </p><ul><li>Advantages: <ul><li>well-deﬁned</li><li>reproducible</li></ul></li><li>Disadvantages: <ul><li>require a human-written reference summary or summaries for comparison, completely disregarding the original document text.</li><li>ROUGE method is limited to measuring a mechanical overlap of text tokens with little regard to semantics.</li></ul></li></ul></li><li><p>Human evaluation:</p><ul><li>Advantages:<ul><li>far more meaningful than ROUG</li><li>far more powerful than ROUG</li></ul></li><li>Disadvantages:<ul><li>far less reproducible</li></ul></li></ul></li><li><p>Train a model on document summaries annotated with human quality scores</p><ul><li>Advantages:<ul><li>evaluate summaries without further human involvement</li><li>achieve high agreement with human labelers</li><li>objective and reproducible</li></ul></li><li>Disadvantages:<ul><li>not generalize 针对特定任务需要针对性训练</li></ul></li></ul></li><li><p>Estimate how “helpful” a summary is for the task of understanding a text</p><ul><li>Disadvantages:<ul><li>one must choose from a vast set of questions one might ask of a text, presupposing knowledge of the document itself and seriously limiting its reproducibility.</li></ul></li></ul></li></ol><h2 id="2-Method"><a href="#2-Method" class="headerlink" title="2. Method"></a>2. Method</h2><h3 id="2-1-Introducing-BLANC"><a href="#2-1-Introducing-BLANC" class="headerlink" title="2.1 Introducing BLANC"></a>2.1 Introducing BLANC</h3><p>​        Consider these ingredients for an ideal summary quality estimator:</p><ol><li>It should measure the functional qualities of a summary as directly as possible. </li><li>It should reliably estimate quality across a broad range of document domains and styles. (Model should be well-documented, well-understood, widely used, and open source.)</li></ol><p>​        The deﬁnition of BLANC: a measure of how well a summary helps an independent pre-trained language model while it performs its language understanding task on a document.  (focusing on the masked token task, also known as the Cloze task(完形填空任务))</p><p>​        BLANC scores should strongly correlate with the summary quality scores of human judges, even when split into multiple quality dimensions. These are the key intuitions:</p><ol><li><p>A more informative summary should help a language model with tasks such as sentence entailment prediction and masked token reconstruction.（一个信息丰富的摘要应该有助于语言模型完成句子蕴涵预测和掩蔽标记重构等任务。）</p></li><li><p>A less factually correct summary should be less helpful to a language model performing inference on the document’s text.（不太符合事实的摘要对用于文本推断的语言模型帮助不大。）</p></li><li><p>A more ﬂuent summary should be better understood by a language model because it better matches the distribution of text on which it was pre-trained.（语言模型应该能够更好地理解更流畅的摘要，因为它能够更好地匹配预先训练过的文本的分布）</p></li></ol><p>​        We explore two versions of BLANC: BLANC-help and a BLANC-tune.The essential difference between them:</p><ol><li><p>BLANC-help uses the summary text by directly concatenating it to each document sentence during inference.</p></li><li><p>BLANC-tune uses the summary text to ﬁne-tune the language model, and then processes the entire document.</p></li></ol><p>Thus with BLANC-help, the language model refers to the summary each time it attempts to understand a part of the document text. While with BLANC-tune, the model learns from the summary ﬁrst, and then uses its gained skill to help it understand the entire document.(在blanco -help中，每当试图理解文档文本的一部分时，语言模型都会引用摘要。使用blanco -tune时，模型首先从摘要中学习，然后使用获得的技能帮助它理解整个文档。)</p><h3 id="2-2-BLANC-help"><a href="#2-2-BLANC-help" class="headerlink" title="2.2 BLANC-help"></a>2.2 BLANC-help</h3><img src="/2020/03/01/Report-Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries/1.png" title="avatar"><p>​        There are many possible choices for how to mask the tokens. Our aim is to mask approximately 15% of tokens in a sentence, and evenly cover all tokens.The unmasking is done twice for each sentence of the text and for each allowed choice of masked tokens in the sentence. First, the unmasking is done for input composed of the summary concatenated with the sentence. Second, the unmasking is done for input composed of a “ﬁller” concatenated with the sentence. The ﬁller has exactly the same lengths as the summary, but each summary token is replaced by a period symbol (“.”).</p><h3 id="2-3-BLANC-tune"><a href="#2-3-BLANC-tune" class="headerlink" title="2.3 BLANC-tune"></a>2.3 BLANC-tune</h3><img src="/2020/03/01/Report-Fill-in-the-BLANC-Human-free-Quality-Estimation-of-Document-Summaries/2.png" title="avatar"><p>​        As in the case of BLANC-help, we deﬁne BLANC-tune by comparing the accuracy of two reconstructions: one that does use the summary, and another that does not. In the case of BLANC-help, this was the difference between placing the summary vs. placing the ﬁller in front of a sentence. Now, in the case of BLANC-tune, we compare the performance of a model ﬁne-tuned on the summary text vs. a model that has never seen the summary.</p><h3 id="2-4-Extractive-summaries-no-copy-pair-guard"><a href="#2-4-Extractive-summaries-no-copy-pair-guard" class="headerlink" title="2.4 Extractive summaries: no-copy-pair guard"></a>2.4 Extractive summaries: no-copy-pair guard</h3><p>​        In the case of purely extractive summaries, the process of calculating BLANC scores may pair a summary with sentences from the text that have been copied into the summary. This exact sentence copying should be unfairly helpful in unmasking words in the original sentence.</p><p>​        We may exclude any pairing of exact copy sentences from the calculation of the measure.</p><ol><li><p>In the process of iterating over text sentences, whenever a sentence contains its exact copy in the summary, it is skipped.</p></li><li><p>In the process of iterating over text sentences, whenever a sentence contains its exact copy in the summary, the copy of the sentence is removed from the summary (only for this speciﬁc step in the process).</p></li></ol><h2 id="3-Comparison-with-human-evaluation-scores"><a href="#3-Comparison-with-human-evaluation-scores" class="headerlink" title="3. Comparison with human evaluation scores"></a>3. Comparison with human evaluation scores</h2><p>​        The BLANC measures do not require any “gold-labeled” data: No human-written summaries nor human-annotated quality scores are needed. Theoretically, the measures should reﬂect how ﬂuent, informative, and factually correct a summary is, simply because only ﬂuent, informative, correct summaries are helpful to the underlying language model.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
            <tag> Estimation Method </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: A Survey on Knowledge Graphs- Representation, Acquisition and Applications</title>
      <link href="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/"/>
      <url>/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications"><a href="#A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications" class="headerlink" title="A Survey on Knowledge Graphs- Representation, Acquisition and Applications"></a>A Survey on Knowledge Graphs- Representation, Acquisition and Applications</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        本文对知识图谱进行了全面的综述，主要涵盖了四个方面：<strong>知识图谱表示学习（knowledge graph representation learning）、知识获取与补全（knowledge acquisition and completion）、时序知识图谱（temporal knowledge graph）、知识感知应用（knowledge-aware applications）</strong>。知识图谱嵌入从表示空间（representation space）、得分函数（scoring function）、编码模型（encoding models）和辅助信息（auxiliary information）四个方面进行组织。另外整理了一些筛选后的数据集和开源库。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>​        知识图谱是事实的结构化表示，由实体、关系和语义描述组成。实体可以是现实世界的对象和抽象概念，关系表示实体之间的关联，实体及其关系的语义描述包含定义良好的类型和属性。属性图或性质图被广泛使用，其中节点和关系具有属性或性质。</p><p>​        <strong>知识图谱</strong>与<strong>知识库</strong>是同义的，只是略有不同。当考虑知识图谱的图结构时，知识图谱可以看作是一个图。当它涉及到形式语义时，它可以作为解释和推断事实的知识库。知识库实例和知识图谱如图1所示。知识可以用事实的三元组形式来表达（头实体，关系，尾实体）或者（主语，谓语，宾语）(head, relation,tail)或 (subject, predicate,object) 。</p><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/1.png" title="avatar"><p>​        基于知识图谱的研究主要集中在知识表示学习(KRL)和知识图谱嵌入(KGE)两个方面。具体的知识获取任务包括知识图谱补全(KGC)、三元组分类、实体识别和关系提取。</p><h2 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h2><h3 id="2-1-A-Brief-History-of-Knowledge-Bases"><a href="#2-1-A-Brief-History-of-Knowledge-Bases" class="headerlink" title="2.1 A Brief History of Knowledge Bases"></a>2.1 A Brief History of Knowledge Bases</h3><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/2.png" title="avatar"><h3 id="2-2-Definitions-and-Notations"><a href="#2-2-Definitions-and-Notations" class="headerlink" title="2.2 Definitions and Notations"></a>2.2 Definitions and Notations</h3><ol><li><strong>定义1：</strong> 知识图谱获取信息并将其集成到本体中，应用推理引擎获得新知识。</li><li><strong>定义2：</strong>知识图谱是由实体和关系构成的多关系图，实体和关系分别被视为节点和不同类型的边。</li></ol><h3 id="2-3-Categorization-of-Research-on-Knowledge-Graph"><a href="#2-3-Categorization-of-Research-on-Knowledge-Graph" class="headerlink" title="2.3 Categorization of Research on Knowledge Graph"></a>2.3 Categorization of Research on Knowledge Graph</h3><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/3.png" title="avatar"><ol><li>知识表示学习(Knowledge Representation Learning，KRL）</li></ol><p>​        将KRL分为<strong>表示空间、评分函数、编码模型和辅助信息</strong>四个方面，为开发KRL模型提供了清晰的工作流程。具体的内容包括:</p><ul><li>关系和实体所表示的表示空间;</li><li>度量事实三元组似然性的评分函数；</li><li>用于表示和学习关系交互的编码模型;</li><li>嵌入方法所集成的辅助信息。</li></ul><p>​        表示学习包括点向空间、流形、复向量空间、高斯分布和离散空间。评分指标一般分为<strong>基于距离的评分函数和基于相似度匹配的评分函数</strong>。目前的研究集中在编码模型，包括线性/双线性模型，因式分解和神经网络。辅助信息包括文本信息、视觉信息和类型信息。</p><ol start="2"><li>知识获取任务分为三类</li></ol><p>​        关系提取和实体发现。第一个用于扩展现有的知识图谱，而其他两个用于从文本中发现新知识(即关系和实体)。KGC分为以下几类: 基于嵌入的排序、关系路径推理、基于规则的推理和元关系学习。实体发现包括识别、消歧、类型化和对齐。关系提取模型利用了注意力机制、图卷积网络、对抗性训练、强化学习、深度残差学习和迁移学习。</p><ol start="3"><li>时序知识图谱</li></ol><p>​        包含了表示学习的时态信息。对时间嵌入、实体动态、时序关系依赖、时序逻辑推理四个领域进行分类。</p><ol start="4"><li>知识感知应用</li></ol><p>​        包括自然语言理解(NLU)、问题回答、推荐系统和各种真实世界的任务，这些应用注入知识以改进表示学习。</p><h2 id="3-Knowledge-Representation-Learning"><a href="#3-Knowledge-Representation-Learning" class="headerlink" title="3. Knowledge Representation Learning"></a>3. Knowledge Representation Learning</h2><p>​        KRL在文献中也被称为KGE、多关系学习和统计关系学习。（原文：KRL is also known as KGE, multi-relation learning, and statistical relational learning in the literature.）</p><h3 id="3-1-Representation-Space"><a href="#3-1-Representation-Space" class="headerlink" title="3.1 Representation Space"></a>3.1 Representation Space</h3><p>​        表示学习的关键是学习低维分布式嵌入的实体和关系。</p><ol><li>Point-Wise Space：Point-wise Euclidean space用于表示实体和关系，嵌入到向量或矩阵空间中，或捕获其交互关系。 </li><li>Complex Vector Space：复向量空间能够捕获对称和不对称关系。</li><li>Gaussian Distribution：将实体和关系嵌入到多维高斯分布中。</li><li>Manifold and Group：流形是一个拓扑空间，它可以用集合理论定义为具有邻域的一组点，而群是抽象代数中定义的代数结构。</li></ol><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/4.png" title="avatar"><h3 id="3-2-Scoring-Function"><a href="#3-2-Scoring-Function" class="headerlink" title="3.2 Scoring Function"></a>3.2 Scoring Function</h3><p>​        评分函数用于度量事实的可信度，在基于能量的学习框架中也称为能量函数。能量学习的目的是学习能量函数。基于能量的学习目标学习能量函数Eθ(x)参数化θ采取x作为输入,以确保正样本分数高于负样本。</p><p>​        评分函数主要有两种：基于距离的(图4(a))和基于相似性的(图4(b))函数，用于度量事实的合理性。基于距离的评分函数通过计算实体之间的距离来衡量事实的合理度，其中使用较多的是关系为h+r≈t的翻译函数。基于语义相似度的评分方法是通过语义匹配来衡量事实的合理性，通常采用乘法公式，即 $h^T M_r ≈ t^T$ ,转换头尾部附近的实体表示空间。</p><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/5.png" title="avatar"><h3 id="3-3-Encoding-Models"><a href="#3-3-Encoding-Models" class="headerlink" title="3.3 Encoding Models"></a>3.3 Encoding Models</h3><p>​        对实体和关系的交互进行编码的模型:    </p><ol><li>线性模型通过将头部实体投射到接近尾部实体的表示空间中，将关系表示为线性/双线性映射。</li><li>因子分解的目的是将关系数据分解为低秩矩阵进行表示学习。</li><li>神经网络用非线性神经激活和更复杂的网络结构来编码关系数据。</li></ol><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/6.png" title="avatar"><h3 id="3-4-Embedding-with-Auxiliary-Information"><a href="#3-4-Embedding-with-Auxiliary-Information" class="headerlink" title="3.4 Embedding with Auxiliary Information"></a>3.4 Embedding with Auxiliary Information</h3><p>​        为了促进更有效的知识表示，多模态嵌入将诸如文本描述、类型约束、关系路径和视觉信息等外部信息与知识图谱本身结合起来。</p><h3 id="3-5-Summary"><a href="#3-5-Summary" class="headerlink" title="3.5 Summary"></a>3.5 Summary</h3><p>​        开发一个新的KRL模型主要需要解决以下四个问题:</p><ol><li>选择哪个表示空间;</li><li>如何测量特定空间中三元组的合理度;</li><li>采用何种编码模型对关系交互进行建模;</li><li>是否利用辅助信息。</li></ol><h2 id="4-Knowledge-Acquisition"><a href="#4-Knowledge-Acquisition" class="headerlink" title="4. Knowledge Acquisition"></a>4. Knowledge Acquisition</h2><p>​        知识获取的目的是从非结构化文本中构造知识图谱，补全已有的知识图，发现和识别实体和关系。</p><h3 id="4-1-Knowledge-Graph-Completion"><a href="#4-1-Knowledge-Graph-Completion" class="headerlink" title="4.1 Knowledge Graph Completion"></a>4.1 Knowledge Graph Completion</h3><p>​        基于知识图谱不完备性的特点，提出了一种新的知识图谱三元组生成方法。典型的子任务包括链路预测、实体预测和关系预测。</p><p>​        对KGC的初步研究主要集中在学习低维嵌入进行三元组预测。综述中将这些方法称为<strong>基于嵌入的方法</strong>。然而，它们大多数都没有捕捉到多步关系。因此，最近的工作转向探索多步骤的关系路径和合并逻辑规则，分别称为<strong>关系路径推理</strong>和<strong>基于规则的推理</strong>。三元组分类是KGC的一个相关任务，它评估了一个事实三元组分类的正确性。</p><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/7.png" title="avatar"><h3 id="4-2-Entity-Discovery"><a href="#4-2-Entity-Discovery" class="headerlink" title="4.2 Entity Discovery"></a>4.2 Entity Discovery</h3><p>​        将基于实体的知识获取分为几个细分的任务，即实体识别、实体消歧、实体类型和实体对齐。</p><ol><li>Entity Recognition：Entity recognition或者named entity recognition (NER)用于识别文本中的实体。</li><li>Entity Typing：实体类型包括粗糙和精细类型，后者使用树结构化类型目录，作为多类别和多标签分类。</li><li>Entity Disambiguation：Entity disambiguation或者entity linking，是将实体名称连接到知识图谱中特定的实体节点。</li><li>Entity Alignment：实体对齐是融合多个异构知识图谱。</li></ol><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/8.png" title="avatar"><h3 id="4-3-Relation-Extraction"><a href="#4-3-Relation-Extraction" class="headerlink" title="4.3 Relation Extraction"></a>4.3 Relation Extraction</h3><p>​        关系抽取是从纯文本中抽取未知关系事实并将其加入到知识图谱中，是自动构建大规模知识图谱的关键。</p><h3 id="4-4-Summary"><a href="#4-4-Summary" class="headerlink" title="4.4 Summary"></a>4.4 Summary</h3><ol><li><p>知识图谱补全：</p><p>​        完成了现有实体之间缺失的链接，或者推断出给定实体和关系查询的实体。基于嵌入的KGC方法通常依赖于三元组表示学习来捕获语义，并对完成的候选排序。基于嵌入的推理仍然停留在个体关系层面，由于忽略了知识图谱的符号性，缺乏可解释性，使得复杂推理能力较差。符号学与嵌入相结合的混合方法结合了基于规则的推理，克服了知识图谱的稀疏性，提高了嵌入的质量，促使有效的规则注入，并引入了可解释的规则。</p></li><li><p>实体发现：</p><p>​        从文本中获取面向实体的知识，将知识融合到知识图谱中。</p></li><li><p>关系抽取：</p><p>​        在距离监督的假设下存在噪声模式，尤其是在不同领域的文本语料库中。因此，弱监督关系提取对于减轻噪声标记的影响是很重要的。</p></li></ol><h2 id="5-Temporal-Knowledge-Graph"><a href="#5-Temporal-Knowledge-Graph" class="headerlink" title="5. Temporal Knowledge Graph"></a>5. Temporal Knowledge Graph</h2><p>​        当前知识图谱研究多集中在静态知识图上，事实不随时间变化，对知识图谱的时间动态研究较少。然而时间信息是非常重要的，因为结构化的知识只在一个特定的时期内存在，而事实的演变遵循一个时间序列。最近的研究开始将时间信息引入到KRL和KGC中，与之前的静态知识图相比，这被称为时序知识图。同时对时间嵌入和关系嵌入进行了研究。</p><ol><li><p>Temporal Information Embedding</p></li><li><p>Entity Dynamics</p></li><li><p>Temporal Relational Dependency</p></li><li><p>Temporal Logical Reasoning</p></li></ol><h2 id="6-Knowledge-aware-Application"><a href="#6-Knowledge-aware-Application" class="headerlink" title="6. Knowledge-aware Application"></a>6. Knowledge-aware Application</h2><ol><li><p>Natural Language Understanding：知识感知NLU将结构化的知识注入到统一的语义空间中，增强语言表示能力。</p></li><li><p>Question Answering：</p><ul><li>Single-fact QA</li><li>Multi-hop Reasoning</li></ul></li><li><p>Recommender Systems</p></li></ol><h2 id="7-F-UTURE-D-IRECTIONS"><a href="#7-F-UTURE-D-IRECTIONS" class="headerlink" title="7. F UTURE D IRECTIONS"></a>7. F UTURE D IRECTIONS</h2><ol><li>Complex Reasoning 复杂推理</li><li>Uniﬁed Framework 统一框架</li><li>Interpretability 可解释性</li><li>Scalability 可扩展性</li><li>Knowledge Aggregation 知识聚合</li><li>Automatic Construction and Dynamics 自动构建和动态知识图谱</li></ol><h2 id="8-Resources"><a href="#8-Resources" class="headerlink" title="8. Resources"></a>8. Resources</h2><img src="/2020/02/21/Report-A-Survey-on-Knowledge-Graphs-Representation-Acquisition-and-Applications/9.png" title="avatar"><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://mp.weixin.qq.com/s/0f5E82utl-faCpmvrDoPEg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/0f5E82utl-faCpmvrDoPEg</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Knowledge Graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</title>
      <link href="/2020/02/18/Report-HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization/"/>
      <url>/2020/02/18/Report-HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><a href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization" class="headerlink" title="HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"></a>HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</h1><h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><p>​        HIBERT：HIerachical Bidirectional Encoder Representations from Transformers</p><p>​        整体思路：抽取式摘要，借鉴了Transformer的思想，利用大量的无标注数据进行预训练，学习句子和文档的表达，进而进行筛选句子作为摘要内容。</p><h2 id="二、模型"><a href="#二、模型" class="headerlink" title="二、模型"></a>二、模型</h2><img src="/2020/02/18/Report-HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization/1.png" title="avatar"><h3 id="2-1-Document-Representation"><a href="#2-1-Document-Representation" class="headerlink" title="2.1 Document Representation"></a>2.1 Document Representation</h3><p>​        为了获取文档的表达，这里使用了两个编码器。一个是句子编码器，用于转换文档中的句子；另一个是文档编码器，根据上下文语句学习句子表达。两者都使用Transformer，每一个词向量随机初始化，并且采用了sine-cosine位置编码；其次，Transformer会将词编码转换成一个高层表达序列，默认最后一个隐藏层表元素作为句子表示，比如<eos>位置的表示；接着每个句子的表示，都会加上一个位置编码，这个位置编码与句子中词语的位置编码相同；最后，获取结合上下文信息的句子表示。</eos></p><h3 id="2-2-Pre-training"><a href="#2-2-Pre-training" class="headerlink" title="2.2 Pre-training"></a>2.2 Pre-training</h3><p>使用双向信息，大致分两步：</p><ol><li>Document Masking</li></ol><p>​    以15%的概率Mask句子，一个被Masked的句子，会做如下处理，三种方式中选择一种：80%的语句会将其中的每一个词语包括标点都转换成[Mask]标识；10%的语句会保持不变，这种方式能够模拟测试时不Mask的情况；10%的语句会随机替换成其他语句，这种方式可以添加一些噪音，提高模型的鲁棒性。</p><ol start="2"><li>Sentence Prediction</li></ol><p>​    被Masked的语句的索引会被记录下来。首先，使用前面的编码器获得语句的上下文信息表示；在预测时，一个一个词语的进行预测，第一个默认是<bos>，在第j步时，输入前j-1个词语的信息以及该句子的上下文信息表示。这里与原来的Transformer稍有不同，原来使用编码器和解码器中的上下文信息表示，而这里只是用解码器中的上下文信息。</bos></p><img src="/2020/02/18/Report-HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization/2.png" title="avatar"><h3 id="2-3-Extractive-Summarization"><a href="#2-3-Extractive-Summarization" class="headerlink" title="2.3 Extractive Summarization"></a>2.3 Extractive Summarization</h3><p>​        建模成序列标注问题，在编码器后加一个线性层和一个SoftMax。</p><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>​        模型训练大致分为三个阶段：第一个阶段，开放域的预训练，使用大量的无特定领域的无标注数据；第二个阶段，特定域的预训练，使用专门数据集进行预训练，比如用于文本摘要的数据集；第三个阶段，微调HIBERT进行语句序列标注的预测。</p><p>​        实验效果不错。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings</title>
      <link href="/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/"/>
      <url>/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/</url>
      
        <content type="html"><![CDATA[<h1 id="STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings"><a href="#STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings" class="headerlink" title="STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings"></a>STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>​STRASS: Summarization by TRAnsformation Selection and Scoring</p><p>整体思路：抽取式摘要，选择句子Embedding与文档Embedding最接近的句子。模型会学习一个文档Embedding的转换，并最大化抽取摘要与真实摘要的相似度。</p><h2 id="二、相关工作"><a href="#二、相关工作" class="headerlink" title="二、相关工作"></a>二、相关工作</h2><p>​        摘要分成两类：生成式摘要和抽取式摘要。</p><ol><li>生成式摘要：生成新的文本来概括文档。可以建模成Seq-Seq的问题。典型模型有PGN。</li><li>抽取式摘要：两类方法解决，一个是序列标注，标注是否作为摘要的一部分；另一个是排序，越重要的句子排名越高。</li><li>两者结合：先抽取式选择句子，再用生成式方法重写他们。</li></ol><h2 id="三、模型"><a href="#三、模型" class="headerlink" title="三、模型"></a>三、模型</h2><p>​        模型总体分为四步：</p><img src="/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/1.png" title="avatar"><p>​        第一步，利用单层MLP将文档Embedding转换成特定形式；</p><p>​        第二步，句子选择生成摘要，句子选择时会提供一个阈值；</p><img src="/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/2.png" title="avatar"><p>​        第三步，生成的摘要的近似表示；</p><img src="/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/3.png" title="avatar"><p>​        第四步，计算与真实摘要的相似度，再反向传播。这里还考虑了一个压缩比的问题，系数越大，越倾向于生成短摘要。</p><img src="/2020/02/18/Report-STRASS-A-Light-and-Effective-Method-for-Extractive-Summarization-Based-on-Sentence-Embeddings/4.png" title="avatar"><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>​        缺点：不能处理多主题文档的摘要问题；没有考虑摘要句子的位置信息。</p><p>​        优点：速度快，CPU就能够训练。</p><p>​        提出了一个新的CASS的法语数据集。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Sentence Centrality Revisited for Unsupervised Summarization</title>
      <link href="/2020/02/18/Report-Sentence-Centrality-Revisited-for-Unsupervised-Summarization/"/>
      <url>/2020/02/18/Report-Sentence-Centrality-Revisited-for-Unsupervised-Summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="Sentence-Centrality-Revisited-for-Unsupervised-Summarization"><a href="#Sentence-Centrality-Revisited-for-Unsupervised-Summarization" class="headerlink" title="Sentence Centrality Revisited for Unsupervised Summarization"></a>Sentence Centrality Revisited for Unsupervised Summarization</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>​        PACSUM：Position-Augmented Centrality based Summarization</p><p>​        整体思路：抽取式摘要，总体还是基于图的排序算法，调整了句子中心度的计算方法。使用BERT来进行句子表示，可以更好地捕获句子意义；将原来的无向图换成有向图，将一条无向边转换成两条有向边。</p><h2 id="二、模型"><a href="#二、模型" class="headerlink" title="二、模型"></a>二、模型</h2><p>​        模型总体分为两大部分：有向图构建和句子相似度计算。</p><h3 id="1-有向图"><a href="#1-有向图" class="headerlink" title="1. 有向图"></a>1. 有向图</h3><p>​        将原来的无向图换成有向图，将一条无向边转换成两条有向边，只是在原来相似度的基础上分别乘以一个不同的系数，这是基于两个句子的连接对两者中心度的贡献是受他们相对位置的影响的，越靠前的句子中心度越高。句子中心度的计算公式如下：</p><img src="/2020/02/18/Report-Sentence-Centrality-Revisited-for-Unsupervised-Summarization/1.png" title="avatar"><p>​        另外，为了减少超参数数量，令两个系数之和等于1。而且第一个系数偏向于负数，说明，与前面句子的相似度会降低该句子的中心度。</p><h3 id="2-相似度计算"><a href="#2-相似度计算" class="headerlink" title="2. 相似度计算"></a>2. 相似度计算</h3><p>​        第一步：利用微调的BERT对句子进行编码；</p><p>​        第二步：为了微调BERT，使用了一个句子级的分布假设作为微调的目标函数。借鉴负采样的思想，将特定句子的前一句和后一句作为正例，语料中的其他句子作为负例。目标函数如下：</p><img src="/2020/02/18/Report-Sentence-Centrality-Revisited-for-Unsupervised-Summarization/2.png" title="avatar"><p>其中，两个向量表示是参数不同的BERT得到的。</p><p>​        第三步：相似度直接有两个向量的点乘得到，点乘的效果优于余弦相似度。Beta系数决定了相似度低于多少才被设置为0。</p><img src="/2020/02/18/Report-Sentence-Centrality-Revisited-for-Unsupervised-Summarization/3.png" title="avatar"><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>​        代码：<a href="https://github.com/mswellhao/PacSum">https://github.com/mswellhao/PacSum</a></p><p>​        实验效果很好，还没来得及看代码，有兴趣看一下。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report: Self-Supervised Learning for Contextualized Extractive Summarization</title>
      <link href="/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/"/>
      <url>/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="Self-Supervised-Learning-for-Contextualized-Extractive-Summarization"><a href="#Self-Supervised-Learning-for-Contextualized-Extractive-Summarization" class="headerlink" title="Self-Supervised Learning for Contextualized Extractive Summarization"></a>Self-Supervised Learning for Contextualized Extractive Summarization</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>​        引入三个辅助预训练方法Mask、Replace、Switch，以自监督的方式来捕获文档级的上下文信息，属于抽取式摘要方式。</p><p>​        代码：https:// github.com/hongwang600/Summarization</p><h2 id="二、模型"><a href="#二、模型" class="headerlink" title="二、模型"></a>二、模型</h2><p>​        这里使用了一个基础模型：一个句子编码器和一个文档级的自注意力机制。句子编码器是一个双向的LSTM，输入是句子序列的Embedding，输出是句子表示；再经过Self-Attention层，输出学习了全局上下文信息的句子表示；再通过线性层判断是否选择该句子。</p><img src="/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/1.png" title="avatar"><h2 id="三、预训练方法"><a href="#三、预训练方法" class="headerlink" title="三、预训练方法"></a>三、预训练方法</h2><ol><li><p>Mask</p><p>​        Mask任务就是从候选池中预测Masked的句子。首先，以一定的概率Mask文档中的句子，并将Masked句子存入候选池。其次，用相同的句子编码器获取候选池中句子的表示。接着，模型需要预测每一个Mask位置缺失的句子，会先将Masked句子用<unk>代替，并计算其结合上下文信息的表示。最后，再计算两个表示的余弦相似度。</unk></p><img src="/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/2.png" title="avatar"><p>​        为了训练模型，使用排序的损失函数来最大化预测句子与其他句子的边距。</p><img src="/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/3.png" title="avatar"></li><li><p>Replace</p><p>​        以一定的概率用其他文档的句子替换本文档中的句子，并使用一个线性层来预测句子是否被替换。</p><img src="/2020/02/17/Report-Self-Supervised-Learning-for-Contextualized-Extractive-Summarization/4.png" title="avatar"></li><li><p>Switch</p><p>​        与Replace类似，只不过是与同一文档中的句子进行交换。</p></li></ol><h2 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h2><p>​        效果的确提升了，而且收敛速度更快。Switch方法的表现最好，但是感觉没有达到读者预想的目标。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report:Video Skimming: Taxonomy and Comprehensive Survey</title>
      <link href="/2020/02/17/Report-Video-Skimming-Taxonomy-and-Comprehensive-Survey/"/>
      <url>/2020/02/17/Report-Video-Skimming-Taxonomy-and-Comprehensive-Survey/</url>
      
        <content type="html"><![CDATA[<h1 id="Video-Skimming-Taxonomy-and-Comprehensive-Survey"><a href="#Video-Skimming-Taxonomy-and-Comprehensive-Survey" class="headerlink" title="Video Skimming: Taxonomy and Comprehensive Survey"></a>Video Skimming: Taxonomy and Comprehensive Survey</h1><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>​        Video Skimming视频略读，又称为动态视频摘要。静态视频摘要是指提取视频中的关键帧，而动态视频摘要是生成短视频。可以利用内部信息和外部信息。内部信息是指视频、音频和文本；外部信息是指用户评论、用户打分、视频回顾（类似于影评）。</p><h2 id="二、系统架构"><a href="#二、系统架构" class="headerlink" title="二、系统架构"></a>二、系统架构</h2><img src="/2020/02/17/Report-Video-Skimming-Taxonomy-and-Comprehensive-Survey/1.png" title="avatar"><ol><li><p>Segmentation：将当前视频分成更小的单元，称为skim     unit。将视频分成最小的可理解单元，并单独处理。最小的可理解单元是指可以传达特定含义的帧数最少的单元。</p></li><li><p>Importance computation：计算skim unit的重要性。</p></li><li><p>User preferences：设定用户需求，比如skim长度、skim类型（强调或者总括）。</p></li><li><p>Skim unit selection：选择需要的skim unit，去重。</p></li></ol><h2 id="三、分类"><a href="#三、分类" class="headerlink" title="三、分类"></a>三、分类</h2><ol><li>不同领域的视频摘要说明以及评估标准：</li></ol><img src="/2020/02/17/Report-Video-Skimming-Taxonomy-and-Comprehensive-Survey/2.png" title="avatar"><ol start="2"><li>不同类型的视频摘要的发展趋势：</li></ol><img src="/2020/02/17/Report-Video-Skimming-Taxonomy-and-Comprehensive-Survey/3.png" title="avatar"><ol start="3"><li>论文中还有详细的数据集划分。</li></ol><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><h4 id="挑战："><a href="#挑战：" class="headerlink" title="挑战："></a>挑战：</h4><ol><li>深度学习方法缺乏大量的训练数据；</li><li>长视频摘要问题；</li><li>实时摘要问题；</li><li>多模态或者跨媒体摘要问题。</li></ol><h4 id="未来方向："><a href="#未来方向：" class="headerlink" title="未来方向："></a>未来方向：</h4><ol><li>确定最佳摘要长度；</li><li>视频摘要的可理解性，确定最小视频单元的时间长度；</li><li>通用领域的视频摘要问题；</li><li>为其他任务合成数据集；</li><li>评估标准的问题。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Video Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report:A Simple Theoretical Model of Importance for Summarization</title>
      <link href="/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/"/>
      <url>/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Simple-Theoretical-Model-of-Importance-for-Summarization"><a href="#A-Simple-Theoretical-Model-of-Importance-for-Summarization" class="headerlink" title="A Simple Theoretical Model of Importance for Summarization"></a>A Simple Theoretical Model of Importance for Summarization</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>​        整体思路：摘要的主要目的就是在损失最小信息量的情况下，最大限度表达原文信息量，因此基于信息论来研究摘要任务是合适的。然而信息论着重于研究信息的不确定性，容易忽略语言中的语义信息，因此直接应用信息论不适合。论文里将文本切分成最基本的语义单元，语义单元负责语义部分，而信息论只需要关注由语义单元构成的文本信息即可。（语义单元可以是字符、词、n-gram、具有更复杂语义语法内容的单元，也称为原子信息块。）文本则是用这些基本语义单元的概率分布来表示。</p><h2 id="二、框架"><a href="#二、框架" class="headerlink" title="二、框架"></a>二、框架</h2><p>​        论文从四个不同的角度，在本质上对摘要本身做了分析。分别是冗余度(redundancy)，相关性(relevance)，informativeness，重要性(importance)。其中，重要性是论文新突出的理念，它结合了其余三个概念的内容，并进行了公式化。</p><h3 id="1-相关性Relevance"><a href="#1-相关性Relevance" class="headerlink" title="1. 相关性Relevance"></a>1. 相关性Relevance</h3><p>​        目前大部分模型对摘要抽取或生成的目标都可近似为相关性。对于有监督学习训练来说，抽取式摘要的训练数据标注了哪些句子是摘要句。最后任务转化为对每个句子做二分类问题，而生成式摘要的seq2seq模型中，也是与标注的人工摘要进行语义单元上的差异计算。对于无监督学习来说，大部分的方法的建模目标都是相关性。</p><p>​        通过阅读摘要，应该降低对原文的不确定感，摘要文本应当以最小的信息损失来推断原文文档。从统计学角度来看，摘要和原文档都各自满足一定的概率分布，而分布之间的接近程度可以简单的使用交叉熵（cross-entropy）衡量。CE指的是交叉熵的函数，注意到这里有个负号，因为交叉熵越小，表示摘要和文档的差异越小，那么相关性应当越强。<br>​<br>​                                                                            REL(S,D) = −CE(S,D)</p><h3 id="2-冗余度Redundancy"><a href="#2-冗余度Redundancy" class="headerlink" title="2. 冗余度Redundancy"></a>2. 冗余度Redundancy</h3><p>​        如果简单的使用相关性来对文档中的句子进行排序，然后选择相关性最高的某些句子来生成摘要，但由于相关性分数接近的句子表述的内容通常也是接近的，因此摘要的冗余度就会很高。而一个好的摘要应该是包含不同的信息，而不是大量相似的信息，而冗余度可以使用熵进行描述。</p><img src="/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/1.png" title="avatar"><p>​        那么冗余度可以表示为Red(S)=−H(S) 。建模的目标是冗余度尽量小，那么表示S SS的熵值越大，表示文本的不确定性越好，所包含的信息量也越大，对应的冗余度也就越小。</p><h3 id="3-信息量Informativeness"><a href="#3-信息量Informativeness" class="headerlink" title="3. 信息量Informativeness"></a>3. 信息量Informativeness</h3><p>​        根据论文的叙述，这个概念假设当前有一个背景知识库K，此时需要对文档D进行摘要抽取，那么候选摘要S对于K来说，应当新增尽可能多的信息，才能让读者在阅读摘要后获取最多的新信息。如果摘要句子说的都是用户早就知道的事情，那么阅读摘要没有给用户产生任何价值。</p><p>​        相关性和冗余度只是在当前处理文档的范围内进行建模，但是人类的语言是有庞大的常识库的。只使用相关性和冗余度有其局限性，因此才引入了informativeness的概念。那么如何度量这个概念呢？informativeness的目标是让S尽可能与K不同，同时K也是由语义单元组成的文本语料集合，因此也可以用Pk来表示K的概率分布，与相关性类似，使用交叉熵来衡量两个概率分布的差异性。</p><img src="/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/2.png" title="avatar"><h3 id="4-重要性Importance"><a href="#4-重要性Importance" class="headerlink" title="4. 重要性Importance"></a>4. 重要性Importance</h3><p>​        针对的是语义单元，目标是计算每个语义单元的重要性分数，在构造摘要时，根据每个语义单元的评分来丢弃不需要的语义单元。</p><img src="/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/3.png" title="avatar"><h3 id="5-整合四个维度"><a href="#5-整合四个维度" class="headerlink" title="5. 整合四个维度"></a>5. 整合四个维度</h3><img src="/2020/02/16/Report-A-Simple-Theoretical-Model-of-Importance-for-Summarization/4.png" title="avatar"><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>​        纯理论论文，没有具体模型，定性分析文本摘要的生成与评估。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/76492696" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76492696</a></li><li><a href="https://blog.csdn.net/Forlogen/article/details/98963499" target="_blank" rel="noopener">https://blog.csdn.net/Forlogen/article/details/98963499</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Report:A Study of Human Summaries of Scientific Articles</title>
      <link href="/2020/02/16/Report-A-Study-of-Human-Summaries-of-Scientific-Articles/"/>
      <url>/2020/02/16/Report-A-Study-of-Human-Summaries-of-Scientific-Articles/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Study-of-Human-Summaries-of-Scientific-Articles"><a href="#A-Study-of-Human-Summaries-of-Scientific-Articles" class="headerlink" title="A Study of Human Summaries of Scientific Articles"></a>A Study of Human Summaries of Scientific Articles</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>What the differences between human summaries and automatic summarizations?</p><ol><li><p>Human summaries have deeper insights, which can be used to imporve and adapt existing automatic summarization systems to the domian of scientific papers.</p></li><li><p>Human summaries tend to be long, detailed and contain headlines and figures from the origin papers.</p></li></ol><p>​Automatic summarization focuses on:</p><ol><li>automatic generation of relatively short summaries(150 - 200 words);</li><li>have an abstract-like structure, lacking other summarization constructs used by humans such as headlines and figures.;</li><li>most existing summarization methods of scientific papers rely on citations in order to pinpoint the import parts, but the citations volume of newly published papers is not enough to perform a similar analysis.</li></ol><p>Dataset for automatic scientific summarization: Scisumm, ScisummNet.</p><p>To solve the above problems, this paper studies a dataset for scientific summarizations, based on long human summaries authored by ShortScience.org users. The goal is to study the characteristics of human scientific summaries and propose to use the summaries published on blogs as a potential benchmark for automation. </p><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h2><p>ShortScience is an open platform for publishing summaries of scientific papers in the domains of Computer Science, Physics and Biology. The website provides minimal instructions on how to write a summary and there is a large variation in summary length and structure.</p><p>How to process?</p><ol><li>Fetch 561 summaries associated with 491 papers;</li><li>Papers are from Arxiv, NeurIPS, ACL, Springer;</li><li>Utilize NLTK for word tokenization and sentence segmentation;</li><li>Use <a href="https://github.com/allenai/science-parse">Science-Parse</a> to extract the PDF text and outputs a Json record containing abstract text, metadata(such as authors and year), and a flat llist of the article sections;</li><li>Disregard sentences less than 20 characters, to minimize effect of parsing errors;</li><li>The mean summary length is 447 words, and the median is 312 words;</li><li>The average number of sentences per summary is 22.</li></ol><h2 id="3-Human-Summaries-Analysis"><a href="#3-Human-Summaries-Analysis" class="headerlink" title="3. Human Summaries Analysis"></a>3. Human Summaries Analysis</h2><h3 id="3-1-Summary-subjectivity"><a href="#3-1-Summary-subjectivity" class="headerlink" title="3.1 Summary subjectivity"></a>3.1 Summary subjectivity</h3><p>For assessing to what extent the summaries represent a subjective account of the origin scientific work:</p><ol><li>extract all sentences containing terms “i” or “my”;</li><li>130 summaries out of 561 summaries include such sentences;</li><li>5 cases errorneous, 53% neutral, 32% positive, 15% negative.</li></ol><p>When decide to publicly express their opinion on scientific work, they tend to present a positive or balanced view and not to criticize. People choose to summarize papers they deem valuable.</p><h3 id="3-2-Summary-coverage"><a href="#3-2-Summary-coverage" class="headerlink" title="3.2 Summary coverage"></a>3.2 Summary coverage</h3><p>To asses to what extent human summaries cover logical aspects of the papers:</p><ol><li>align each summary sentence to the sentence in the original paper most similar to it and with the category of that sentence;</li><li>paper sections hierarchy was restored and sub-section are merged into their containing high level section;</li><li>high level sections are as follows: Introduction, Related work, Method, Results, Experiments, Discussion, Conclusions, Future work, Unknown; (2051 out of 3421 article sections were assighed with a category while the rest were classified as Unknown)</li><li>section sentences inhert their containing section title;</li><li>experiment with three similarity methods: <ul><li>ROUGE-L</li><li>average of F1, ROUGE-1, ROUGE21 and ROUGE-L</li><li>cosine similarity over word vectors</li></ul></li></ol><img src="/2020/02/16/Report-A-Study-of-Human-Summaries-of-Scientific-Articles/1.png" title="avatar"><p>The weights are quite stable when using different similarities. A summarization algorithm can aim at assigning higher focus to more salient logical sections, reﬂecting how humans attend different sections in their summary.</p><h3 id="3-3-Summary-style"><a href="#3-3-Summary-style" class="headerlink" title="3.3 Summary style"></a>3.3 Summary style</h3><h4 id="3-3-1-Figures-inclusion"><a href="#3-3-1-Figures-inclusion" class="headerlink" title="3.3.1 Figures inclusion"></a>3.3.1 Figures inclusion</h4><p>Some human summaries include ﬁgures from the original paper including image captures of equations or tables. About 31% of the summaries include at least one such ﬁgure, with an average of 2 ﬁgures per summary.</p><p>We need to consider multi-modal summarization and no work now.</p><h4 id="3-3-2-Summary-Itemization"><a href="#3-3-2-Summary-Itemization" class="headerlink" title="3.3.2 Summary Itemization"></a>3.3.2 Summary Itemization</h4><p>Almost half of the summaries utilized some form of structuring using itemization (i.e., bullets or numbering)(编号或符号，代表逐条记录):</p><ol><li>with an average of 15 items per summary;</li><li>The average size of an item is 2 sentences. </li></ol><h4 id="3-3-3-Headlines"><a href="#3-3-3-Headlines" class="headerlink" title="3.3.3 Headlines"></a>3.3.3 Headlines</h4><p>About 35% of the summaries contain lines that start with “#”, which act as summary “headlines”.</p><img src="/2020/02/16/Report-A-Study-of-Human-Summaries-of-Scientific-Articles/2.png" title="avatar">]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Text Summarization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python使用json.dump的中文编码问题</title>
      <link href="/2019/10/10/python%E4%BD%BF%E7%94%A8json-dump%E7%9A%84%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/"/>
      <url>/2019/10/10/python%E4%BD%BF%E7%94%A8json-dump%E7%9A%84%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>当json中有中文字符串时，直接<em>dump</em>会出错，需要在<em>open</em>函数中加上<em>encoding=‘utf-8’</em>，<em>dump</em>函数中加上<em>ensure_ascii=False</em>。  </p><img src="/2019/10/10/python使用json-dump的中文编码问题/1.png" title="avatar">]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux安装JDK</title>
      <link href="/2019/09/28/Linux%E5%AE%89%E8%A3%85JDK/"/>
      <url>/2019/09/28/Linux%E5%AE%89%E8%A3%85JDK/</url>
      
        <content type="html"><![CDATA[<ol><li><p>先将jdk1.7的安装包下载到本地，在上传到Linux；</p></li><li><p>tar zxvf  jdk-7u80-linux-x64.tar.gz</p></li><li><p>vim ~/.bashrc</p></li><li><p>export JAVA_HOME=/home/wangmeng/jdk1.7.0_80<br>export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:<br>$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>export PATH=$PATH:$JAVA_HOME/bin</p></li><li><p>source ~/.bashrc</p></li><li><p>java -version</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual Studio中C++使用Scanf报错</title>
      <link href="/2019/09/27/Visual-Studio%E4%B8%ADC-%E4%BD%BF%E7%94%A8Scanf%E6%8A%A5%E9%94%99/"/>
      <url>/2019/09/27/Visual-Studio%E4%B8%ADC-%E4%BD%BF%E7%94%A8Scanf%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<p>解决办法：  </p><ol><li><p>在新建项目时，取消SDL前的勾号；  </p><img src="/2019/09/27/Visual-Studio中C-使用Scanf报错/1.png" title="avatar"></li><li><p>项目属性;<br>预处理器定义 编辑：增加首项：_CRT_SECURE_NO_WARNINGS  </p><img src="/2019/09/27/Visual-Studio中C-使用Scanf报错/2.png" title="avatar"></li><li><p>可以在代码生成中关闭安全检查。  </p><img src="/2019/09/27/Visual-Studio中C-使用Scanf报错/3.png" title="avatar"></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tex中文乱码</title>
      <link href="/2019/09/26/Tex%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/"/>
      <url>/2019/09/26/Tex%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<p>加上带注释的语句：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\usepackage&#123;amsmath,amssymb&#125;</span><br><span class="line">\usepackage&#123;latexsym&#125;</span><br><span class="line">\usepackage&#123;CJK&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\begin&#123;CJK*&#125;&#123;GBK&#125;&#123;gsbn&#125;       # 需要添加</span><br><span class="line">文本内容</span><br><span class="line"></span><br><span class="line">\end&#123;CJK*&#125;                    # 需要添加</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Windows </tag>
            
            <tag> Tex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tex中找不到.sty文件</title>
      <link href="/2019/09/25/Tex%E4%B8%AD%E6%89%BE%E4%B8%8D%E5%88%B0-sty%E6%96%87%E4%BB%B6/"/>
      <url>/2019/09/25/Tex%E4%B8%AD%E6%89%BE%E4%B8%8D%E5%88%B0-sty%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>问题：Tex编译报错，找不到指定的.sty文件。  </p><p>解决办法：从网上下载对应的sty文件，并拷贝到当前.tex目录下。  </p><p>下载链接：<a href="https://searchcode.com/codesearch/view/31207532/#" target="_blank" rel="noopener">https://searchcode.com/codesearch/view/31207532/#</a>  </p><p>可以在View File Tree中搜索。  </p><img src="/2019/09/25/Tex中找不到-sty文件/1.png" title="avatar">]]></content>
      
      
      
        <tags>
            
            <tag> Windows </tag>
            
            <tag> Tex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WinEdt打开tex文件Reading Error</title>
      <link href="/2019/09/24/WinEdt%E6%89%93%E5%BC%80tex%E6%96%87%E4%BB%B6Reading-Error/"/>
      <url>/2019/09/24/WinEdt%E6%89%93%E5%BC%80tex%E6%96%87%E4%BB%B6Reading-Error/</url>
      
        <content type="html"><![CDATA[<p>错误：WinEdt打开tex文件Reading Error。  </p><p>原因：因为.tex文件中包含了utf-8字符，而在打开的时候并没有指明utf-8打开方式。  </p><p>解决方案：  </p><img src="/2019/09/24/WinEdt打开tex文件Reading-Error/1.png" title="avatar"><p>参考链接：<br><a href="https://blog.csdn.net/garfielder007/article/details/51619821" target="_blank" rel="noopener">https://blog.csdn.net/garfielder007/article/details/51619821</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Windows </tag>
            
            <tag> Tex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows设置Notepad++为txt的默认打开方式</title>
      <link href="/2019/09/23/Windows%E8%AE%BE%E7%BD%AENotepad-%E4%B8%BAtxt%E7%9A%84%E9%BB%98%E8%AE%A4%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/09/23/Windows%E8%AE%BE%E7%BD%AENotepad-%E4%B8%BAtxt%E7%9A%84%E9%BB%98%E8%AE%A4%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<ol><li>win+R，输入regedit打开注册表编辑器；</li><li>定位到 <em>HKEY_LOCAL_MACHINE\SOFTWARE\Classes\txtfile\shell\Open\command</em> 这个注册表下面；</li><li>右边有一个REG_SZ类型数据点右键修改为你的NotePad++安装目录；</li><li>修改数值数据为 (“D:\Notepad++\notepad++.exe %1”)；</li><li>系统默认为自带的NotePad打开<br>%SystemRoot%\system32\NOTEPAD.EXE %1，这个是默认的地址，如果想恢复系统默认情况，修改为此值。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows安装XShell</title>
      <link href="/2019/09/22/Windows%E5%AE%89%E8%A3%85XShell/"/>
      <url>/2019/09/22/Windows%E5%AE%89%E8%A3%85XShell/</url>
      
        <content type="html"><![CDATA[<h1 id="一、下载安装"><a href="#一、下载安装" class="headerlink" title="一、下载安装"></a>一、下载安装</h1><p>可直接从官网下载免费版：<a href="https://www.netsarang.com/zh/" target="_blank" rel="noopener">https://www.netsarang.com/zh/</a>  </p><p>使用邮箱接受下载链接，直接安装。</p><h1 id="二、问题解决"><a href="#二、问题解决" class="headerlink" title="二、问题解决"></a>二、问题解决</h1><p>如果遇到问题，类似只能对当前安装的产品有效，说明之前安装版本没有删除干净，可以到到C盘，C:\Program Files (x86)\InstallShield Installation Information，删掉{F3FDFD5A-A201-407B-887F-399484764ECA}这个文件夹就可以，不一定是那个。可以备份全删，重新安装后在粘贴回去。  </p><p>参考链接：<br><a href="https://blog.csdn.net/qq_17758709/article/details/78920641" target="_blank" rel="noopener">https://blog.csdn.net/qq_17758709/article/details/78920641</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu下安装和使用GLPK</title>
      <link href="/2019/09/21/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8GLPK/"/>
      <url>/2019/09/21/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8GLPK/</url>
      
        <content type="html"><![CDATA[<h1 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h1><blockquote><ul><li>wget <a href="http://ftp.gnu.org/gnu/glpk/glpk-4.65.tar.gz" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/glpk/glpk-4.65.tar.gz</a></li><li>tar -xzvf glpk-4.65.tar.gz</li><li>./configure</li><li>make</li><li>sudo make install</li></ul></blockquote><p>使用前可能还需要修改一下/etc/ld.so.conf.d/libc.conf文件</p><blockquote><ul><li>udo vim /etc/ld.so.conf.d/libc.conf</li><li>/usr/local/lib/  需要有/号在lib后面</li><li>sudo ldconfig /etc/ld.so.conf.d/lib.conf</li><li>sudo ldconfig /etc/ld.so.conf</li></ul></blockquote><h1 id="二、使用："><a href="#二、使用：" class="headerlink" title="二、使用："></a>二、使用：</h1><ol><li>编辑glpsolEx.mod文件： vim glpsolEx.mod</li><li>执行 glpsol -m glpsolEx.mod -o glpsolEx.sol ，将结果输出到 glpsolEx.sol中。</li><li>说明：glpsolEx.mod文件内容如下：  </li></ol><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Variables */</span>  </span><br><span class="line"><span class="keyword">var</span> x1 &gt;= <span class="number">0</span>;  </span><br><span class="line"><span class="keyword">var</span> x2 &gt;= <span class="number">0</span>;</span><br><span class="line"><span class="keyword">var</span> x3 &gt;= <span class="number">0</span>;  </span><br><span class="line"><span class="comment">/* Object function */</span>  </span><br><span class="line">maximize z: x1 + <span class="number">14</span>*x2 + <span class="number">6</span>*x3;  </span><br><span class="line"><span class="comment">/* Constrains */</span>  </span><br><span class="line">s.t. con1: x1 + x2 + x3 &lt;= <span class="number">4</span>;  </span><br><span class="line">s.t. con2: x1  &lt;= <span class="number">2</span>;  </span><br><span class="line">s.t. con3: x3  &lt;= <span class="number">3</span>;  </span><br><span class="line">s.t. con4: <span class="number">3</span>*x2 + x3  &lt;= <span class="number">6</span>;  </span><br><span class="line">end;</span><br></pre></td></tr></table></figure><p>参考链接:  </p><ul><li><a href="http://www.cnblogs.com/jostree/p/4156204.html" target="_blank" rel="noopener">http://www.cnblogs.com/jostree/p/4156204.html</a></li><li><a href="https://blog.csdn.net/danwuxie/article/details/80981169" target="_blank" rel="noopener">https://blog.csdn.net/danwuxie/article/details/80981169</a></li><li><a href="https://blog.csdn.net/zhuoyuezai/article/details/78844549" target="_blank" rel="noopener">https://blog.csdn.net/zhuoyuezai/article/details/78844549</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> GLPK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git上传100MB限制</title>
      <link href="/2019/09/21/Git%E4%B8%8A%E4%BC%A0100MB%E9%99%90%E5%88%B6/"/>
      <url>/2019/09/21/Git%E4%B8%8A%E4%BC%A0100MB%E9%99%90%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>Github只允许上传100MB的文件，如果超过，服务器会直接拒绝。</p><p>解决办法：  </p><ol><li>sudo git rm –cached 文件路径；</li><li>sudo git commit –amend -CHEAD;</li><li>sudo push -u origin master.</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>you-get安装</title>
      <link href="/2019/09/19/you-get%E5%AE%89%E8%A3%85/"/>
      <url>/2019/09/19/you-get%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h1><p>使用<em>pip install you-get</em>命令安装即可。  </p><h1 id="二、视频下载"><a href="#二、视频下载" class="headerlink" title="二、视频下载"></a>二、视频下载</h1><ol><li><p>下载视频<br>打开cmd，输入”you-get+空格+视频url”即可下载。  </p></li><li><p>暂停  </p></li></ol><p><em>Ctrl+C</em>是中断下载任务。下次下载相同的视频时，默认继续上次未完成的下载（前提时未将上次下载的临时.download文件删除，否则重新下载），如已经完成则会自动跳过。  </p><ol start="3"><li>自定义下载名称和保存路径<br>使用-o（小写的o）设置保存路径，-O(大写的O)设置下载文件的名称。<br>例如：<em>you-get -o ./ -O test.mp4 <a href="https://www.youtube.com/example" target="_blank" rel="noopener">https://www.youtube.com/example</a></em>  </li></ol><h1 id="三、批量下载"><a href="#三、批量下载" class="headerlink" title="三、批量下载"></a>三、批量下载</h1><p>you-get支持批量下载，复制的是视频首页或者简介页的地址。</p><h1 id="四、本地播放"><a href="#四、本地播放" class="headerlink" title="四、本地播放"></a>四、本地播放</h1><p>本地播放器播放网页视频：让网页视频在本地播放，没有任何广告和评论部分。  </p><ol><li>进入视频播放器的安装目录：<em>cd D:\PotPlayer</em>  </li><li>输入以下参数：<em>ou-get -p xxx.exe url</em>  </li><li>比如： <em>you-get -p PotPlayerMini64.exe <a href="https://www.iqiyi.com/example.html" target="_blank" rel="noopener">https://www.iqiyi.com/example.html</a></em>  </li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows下IDEA无法识别JDK</title>
      <link href="/2019/09/16/Windows%E4%B8%8BIDEA%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%ABJDK/"/>
      <url>/2019/09/16/Windows%E4%B8%8BIDEA%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%ABJDK/</url>
      
        <content type="html"><![CDATA[<ol><li>首先确认已经安装jdk；</li><li>在IDEA菜单栏中选中File或者在初始界面的configuration中选择Project Structure；</li><li>弹出的对话框左边有一个SDK选项，单击；</li><li>点击+号，选择JDK；</li><li>定位到JDK安装目录下，比如：D:\Java\jdk1.8.0_91；<br>然后点击Apply，点击OK。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows安装JDK</title>
      <link href="/2019/09/16/Windows%E5%AE%89%E8%A3%85JDK/"/>
      <url>/2019/09/16/Windows%E5%AE%89%E8%A3%85JDK/</url>
      
        <content type="html"><![CDATA[<ol><li><p>下载链接：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a>  </p></li><li><p>修改安装目录；  </p><img src="/2019/09/16/Windows安装JDK/1.png" title="avatar">  </li><li><p>配置环境变量  </p></li></ol><ul><li>安装完JDK后配置环境变量  计算机→属性→高级系统设置→高级→环境变量;  </li><li>系统变量→新建 JAVA_HOME 变量，变量值填写jdk的安装目录（本人是 D:\Java\jdk1.8.0_91)；  </li><li>系统变量→寻找 Path 变量→编辑，在变量值最后输入 %JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;（注意原来Path的变量值末尾有没有;号，如果没有，先输入；号再输入上面的代码）；  </li><li>系统变量→新建 CLASSPATH 变量，变量值填写   .;%JAVA_HOME%\lib;%JAVA_HOME%\lib\tools.jar（注意最前面有一点）；  </li><li>检验是否配置成功 运行cmd 输入 java -version ，若如图所示 显示版本信息 则说明安装和配置成功。  <img src="/2019/09/16/Windows安装JDK/2.png" title="avatar">  </li></ul><ol start="4"><li>使用手册中文版：<a href="https://github.com/judasn/IntelliJ-IDEA-Tutorial">https://github.com/judasn/IntelliJ-IDEA-Tutorial</a>  </li></ol><p>参考链接：<a href="https://jingyan.baidu.com/article/6dad5075d1dc40a123e36ea3.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/6dad5075d1dc40a123e36ea3.html</a>  </p>]]></content>
      
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>brew安装权限问题</title>
      <link href="/2019/09/14/brew%E5%AE%89%E8%A3%85%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98/"/>
      <url>/2019/09/14/brew%E5%AE%89%E8%A3%85%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>brew install wget 出现类似问题：  </p><img src="/2019/09/14/brew安装权限问题/1.jpg" title="avatar">  <p>解决办法：一个一个文件的赋予权限：  </p><img src="/2019/09/14/brew安装权限问题/2.jpg" title="avatar"> ]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> brew </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不同类型的文献检索</title>
      <link href="/2019/09/12/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%87%E7%8C%AE%E6%A3%80%E7%B4%A2/"/>
      <url>/2019/09/12/%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%87%E7%8C%AE%E6%A3%80%E7%B4%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="一、图书类检索"><a href="#一、图书类检索" class="headerlink" title="一、图书类检索"></a>一、图书类检索</h1><ol><li>大英百科全书电子版  <a href="http://academic.eb.com/" target="_blank" rel="noopener">http://academic.eb.com/</a></li><li>Knovel电子工具书  <a href="http://app.knovel.com/web/" target="_blank" rel="noopener">http://app.knovel.com/web/</a></li><li>Springer电子书    <a href="http://link.springer.com/" target="_blank" rel="noopener">http://link.springer.com/</a></li><li>OECD ilibrary</li><li>美国国家学术出版社电子图书   <a href="http://www.nap.edu" target="_blank" rel="noopener">http://www.nap.edu</a></li><li>方正电子图书&amp;方正年鉴工具书  <a href="http://159.226.100.31/usp/" target="_blank" rel="noopener">http://159.226.100.31/usp/</a></li><li>超星电子图书  <a href="http://www.sslibrary.com/home/index" target="_blank" rel="noopener">http://www.sslibrary.com/home/index</a></li></ol><h1 id="二、学位论文"><a href="#二、学位论文" class="headerlink" title="二、学位论文"></a>二、学位论文</h1><ol><li>中国学位论文库 <a href="http://www.wanfangdata.com.cn" target="_blank" rel="noopener">http://www.wanfangdata.com.cn</a>  万方</li><li>中国学位馆藏检索  <a href="http://www.istic.ac.cn/" target="_blank" rel="noopener">http://www.istic.ac.cn/</a></li><li>中国科学院学位论文检索 <a href="http://sdb.las.ac.cn/paper/search_pap.jsp" target="_blank" rel="noopener">http://sdb.las.ac.cn/paper/search_pap.jsp</a></li><li>国家科技图书文献中心（NSTL）学位论文库  <a href="http://www.nstl.gov.cn" target="_blank" rel="noopener">http://www.nstl.gov.cn</a> </li><li>中国优秀博硕士论文全文数据库  <a href="http://www.cnki.net" target="_blank" rel="noopener">http://www.cnki.net</a></li><li>国家图书馆学位论文检索系统   <a href="http://www.nlc.gov.cn" target="_blank" rel="noopener">http://www.nlc.gov.cn</a></li><li>CALIS高校学位论文数据库  <a href="http://etd.calis.edu.cn" target="_blank" rel="noopener">http://etd.calis.edu.cn</a></li><li>CETD中文电子学位论文服务 <a href="http://www.airitilibrary.com/" target="_blank" rel="noopener">http://www.airitilibrary.com/</a></li></ol><h1 id="三、专利"><a href="#三、专利" class="headerlink" title="三、专利"></a>三、专利</h1><ol><li>国家知识产权局   <a href="http://www.sipo.gov.cn/sipo/" target="_blank" rel="noopener">http://www.sipo.gov.cn/sipo/</a></li><li>Derwent Innovation Index(ISI DII) </li><li>欧洲专利局专利数据库  <a href="http://ep.espacenet.com/?locale=en_EP" target="_blank" rel="noopener">http://ep.espacenet.com/?locale=en_EP</a> </li><li>美国专利与商标局专利全文数据库   <a href="http://www.uspto.gov/patft/index.html" target="_blank" rel="noopener">http://www.uspto.gov/patft/index.html</a> </li><li>世界知识产权组织专利检索  <a href="http://www.wipo.int/pctdb/en/" target="_blank" rel="noopener">http://www.wipo.int/pctdb/en/</a></li></ol><h1 id="四、标准"><a href="#四、标准" class="headerlink" title="四、标准"></a>四、标准</h1><h3 id="国内标准检索"><a href="#国内标准检索" class="headerlink" title="国内标准检索"></a>国内标准检索</h3><ol><li>NSTL 中外标准数据库  <a href="http://www.nstl.gov.cn" target="_blank" rel="noopener">http://www.nstl.gov.cn</a> </li><li>国家科学图书馆标准信息检索系统  <a href="http://www.las.ac.cn/standard/standard.jsp" target="_blank" rel="noopener">http://www.las.ac.cn/standard/standard.jsp</a></li><li>万方数据资源系统  <a href="http://www.wanfangdata.com.cn/" target="_blank" rel="noopener">http://www.wanfangdata.com.cn/</a></li><li>中国标准服务网  <a href="http://www.cssn.net.cn" target="_blank" rel="noopener">www.cssn.net.cn</a></li><li>标准网  <a href="http://www.standardcn.com" target="_blank" rel="noopener">www.standardcn.com</a></li><li>国家标准化管理委员会  <a href="http://www.sac.gov.cn/" target="_blank" rel="noopener">http://www.sac.gov.cn/</a> </li><li>国家军用标准化信息网  <a href="http://www.gjb.com.cn/" target="_blank" rel="noopener">http://www.gjb.com.cn/</a> </li></ol><h3 id="国外标准检索"><a href="#国外标准检索" class="headerlink" title="国外标准检索"></a>国外标准检索</h3><ol><li><a href="http://ieeexplore.ieee.org/" target="_blank" rel="noopener">http://ieeexplore.ieee.org/</a></li><li><a href="http://www.iso.org/" target="_blank" rel="noopener">http://www.iso.org/</a></li><li><a href="http://webstore.ansi.org/" target="_blank" rel="noopener">http://webstore.ansi.org/</a></li></ol><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol><li>NSTL 中外标准数据库  <a href="http://www.nstl.gov.cn/index.html" target="_blank" rel="noopener">http://www.nstl.gov.cn/index.html</a></li></ol><h1 id="五、科技报告"><a href="#五、科技报告" class="headerlink" title="五、科技报告"></a>五、科技报告</h1><ol><li>中国国家科技报告服务系统   <a href="http://www.nstrs.cn/" target="_blank" rel="noopener">http://www.nstrs.cn/</a></li><li>中国报告大厅   <a href="http://www.chinabgao.com" target="_blank" rel="noopener">http://www.chinabgao.com</a></li></ol><h1 id="六、数值数据"><a href="#六、数值数据" class="headerlink" title="六、数值数据"></a>六、数值数据</h1><ol><li>国务院发展研究中心信息网  <a href="http://drcnet.las.ac.cn" target="_blank" rel="noopener">http://drcnet.las.ac.cn</a></li><li>中国经济网产业数据库  <a href="http://ceicy.las.ac.cn" target="_blank" rel="noopener">http://ceicy.las.ac.cn</a></li><li>中国经济统计数据库  <a href="http://ceitj.las.ac.cn" target="_blank" rel="noopener">http://ceitj.las.ac.cn</a></li><li>中国资讯行搜数网   <a href="http://soshoo.las.ac.cn/" target="_blank" rel="noopener">http://soshoo.las.ac.cn/</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 文献检索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>修改Jupyter Notebook目录</title>
      <link href="/2019/09/12/%E4%BF%AE%E6%94%B9Jupyter-Notebook%E7%9B%AE%E5%BD%95/"/>
      <url>/2019/09/12/%E4%BF%AE%E6%94%B9Jupyter-Notebook%E7%9B%AE%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<ol><li><p>打开 Anaconda Prompt ，输入:   </p><blockquote><p>jupyter notebook –generate-config  </p></blockquote></li><li><p>找到 jupyter_notebook_config.py 的路径并打此文件;  </p></li><li><p>找到 c.NotebookApp.notebook_dir 这个变量，赋值为： ‘E:\Code\Jupyter-notebook’;  </p><img src="/2019/09/12/修改Jupyter-Notebook目录/1.jpg" title="avatar">  <p>Ubuntu下的修改方法类似。</p></li><li><p>改完后保存，再次通过 Anaconda Navigator 进入 Jupyter Notebook 的时候会发现默认路径已经更改。  </p></li><li><p>如果直接通过 Jupyter Notebook 的快捷方式进入，默认目录还是原来那个。如果需要修改，找到快捷方式，右键打开属性，将“目标” 最后面的 “%USERPROFILE%” 删除就可以了。  </p></li></ol><p>参考链接：  </p><ul><li><a href="https://blog.csdn.net/white_rabbit_2/article/details/83862429" target="_blank" rel="noopener">https://blog.csdn.net/white_rabbit_2/article/details/83862429</a>  </li><li><a href="https://blog.csdn.net/weixin_36292173/article/details/76209839" target="_blank" rel="noopener">https://blog.csdn.net/weixin_36292173/article/details/76209839</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac下Anaconda配置环境变量</title>
      <link href="/2019/09/11/Mac%E4%B8%8BAnaconda%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
      <url>/2019/09/11/Mac%E4%B8%8BAnaconda%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<p>Anaconda安装成功之后，需要配置环境变量。  </p><blockquote><p>vim ~/.zshrc<br>export PATH=”/Applications/anaconda3/bin:$PATH”<br>source ~/.zshrc<br>conda –version   #测试是否配置成功</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> Anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python自动创建requirements.txt</title>
      <link href="/2019/09/10/python%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BArequirements-txt/"/>
      <url>/2019/09/10/python%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BArequirements-txt/</url>
      
        <content type="html"><![CDATA[<h2 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h2><p>第一种方法适用于虚拟环境或者Docker中生成requirements.txt。  </p><blockquote><p>pip freeze &gt; requirements.txt  </p></blockquote><p>安装requirements.txt依赖。  </p><blockquote><p>pip install -r requirements.txt  </p></blockquote><h2 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h2><p>推荐第二种方法，使用pipreqs，导出项目代码中导入的模块。  </p><blockquote><p>pip install pipreqs<br>pipreqs ./CRAWLER_Zhihu/  </p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows安装Mysql</title>
      <link href="/2019/09/10/Windows%E5%AE%89%E8%A3%85Mysql/"/>
      <url>/2019/09/10/Windows%E5%AE%89%E8%A3%85Mysql/</url>
      
        <content type="html"><![CDATA[<ol><li><p>下载链接：<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a><br>本机选择的是Mysql5.7.13 zip版本;  </p></li><li><p>解压mysql-5.7.13-winx64.zip，并移动到安装目录下；</p></li><li><p>添加系统环境变量path，内容为D:\MySQL\mysql-5.7.13-winx64 ;  </p></li><li><p>修改配置文件my-default.ini，添加内容：  </p><blockquote><p>basedir = D:\MySQL\mysql-5.7.13-winx64<br>datadir = D:\MySQL\mysql-5.7.13-winx64\data</p></blockquote></li><li><p>以管理员身份运行cmd，输入cd D:\MySQL\mysql-5.7.13-winx64\bin；</p></li><li><p>输入.\mysqld –initialize-insecure –user=mysql 回车;  </p></li><li><p>输入 .\mysqld install 回车;  </p></li><li><p>输入net start mysql 回车启动mysql服务  管理员权限;  </p></li><li><p>输入.\mysql -u root -p 回车登录mysql数据库，没有密码，直接回车;  </p></li><li><p>测试，输入show databases:  </p><img src="/2019/09/10/Windows安装Mysql/1.jpg" title="avatar">  </li></ol><p>输入quit，退出。  </p><ol start="11"><li>输入net stop mysql，停止mysql程序。</li></ol><p>参考链接:  </p><ul><li><a href="https://www.cnblogs.com/ayyl/p/5978418.html" target="_blank" rel="noopener">https://www.cnblogs.com/ayyl/p/5978418.html</a>  </li><li><a href="https://www.cnblogs.com/reyinever/p/8551977.html" target="_blank" rel="noopener">https://www.cnblogs.com/reyinever/p/8551977.html</a>  </li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Windows </tag>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu安装Mysql</title>
      <link href="/2019/09/10/Ubuntu%E5%AE%89%E8%A3%85Mysql/"/>
      <url>/2019/09/10/Ubuntu%E5%AE%89%E8%A3%85Mysql/</url>
      
        <content type="html"><![CDATA[<h1 id="安装Mysql"><a href="#安装Mysql" class="headerlink" title="安装Mysql"></a>安装Mysql</h1><ol><li>首先安装服务端、客户端以及其他的依赖库；  </li></ol><blockquote><p>sudo apt-get install mysql-server<br>sudo apt install mysql-client<br>sudo apt install libmysqlclient-dev  </p></blockquote><ol start="2"><li>测试是否安装成功。</li></ol><blockquote><p>sudo netstat -tap | grep mysql  </p></blockquote><h1 id="设置允许远程访问"><a href="#设置允许远程访问" class="headerlink" title="设置允许远程访问"></a>设置允许远程访问</h1><ol><li>修改配置文件mysqld.conf；  </li></ol><blockquote><p>sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf  </p></blockquote><p>将文件中的bind-address = 127.0.0.1这一行注释掉。  </p><ol start="2"><li>进入Mysql数据库；  </li></ol><blockquote><p>mysql -u root -p  </p></blockquote><p>进入MySQL命令行表示安装成功。  </p><ol start="3"><li>执行授权命令；  </li></ol><blockquote><p>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’%’ IDENTIFIED BY ‘密码’ WITH GRANT OPTION;  </p></blockquote><p>其中，root是数据库的用户，*表示所有IP都可以登录该数据库。  </p><ol start="4"><li>保存设置；  </li></ol><blockquote><p>flush privileges;  </p></blockquote><ol start="5"><li><p>退出Mysql用exit；  </p></li><li><p>重启Mysql。</p></li></ol><blockquote><p>service mysql restart  </p></blockquote><p>参考链接：<br><a href="https://andy6804tw.github.io/2019/01/29/ubuntu-mysql-setting/#2-%E9%80%B2%E5%85%A5-mysql-%E6%9C%8D%E5%8B%99" target="_blank" rel="noopener">https://andy6804tw.github.io/2019/01/29/ubuntu-mysql-setting/#2-%E9%80%B2%E5%85%A5-mysql-%E6%9C%8D%E5%8B%99</a>  </p>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Ubuntu </tag>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker创建镜像</title>
      <link href="/2019/09/07/Docker%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/"/>
      <url>/2019/09/07/Docker%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<p>Docker创建镜像有两种方式:  </p><ul><li>从已经创建镜像的容器中更新镜像，并且提交这个镜像；</li><li>使用Dockerfile指令来创建镜像。</li></ul><h1 id="一、已有镜像更新"><a href="#一、已有镜像更新" class="headerlink" title="一、已有镜像更新"></a>一、已有镜像更新</h1><ol><li><p>在更新镜像之前，先用镜像创建一个容器；  </p><blockquote><p>docker run -t -i ubuntu:18.04  /bin/bash</p></blockquote></li><li><p>执行上述命令之后，会返回一个Container_ID；可以在伪终端中进行修改，比如：apt-get update； </p></li><li><p>再用exit退出容器；  </p></li><li><p>通过docker commit这个命令来提交修改后的容器副本； </p><blockquote><p>docker   commit   -m=”description”   -a=”author”   Container_ID   author/image:tag</p></blockquote></li><li><p>docker images 就可以查看到这个镜像。</p></li></ol><h1 id="Dockerfile创建镜像"><a href="#Dockerfile创建镜像" class="headerlink" title="Dockerfile创建镜像"></a>Dockerfile创建镜像</h1><ol><li><p>首先需要创建一个Dockerfile文件，包含一组指令，而且每一个指令的前缀必须大写。比如官网示例中填入以下内容：  </p><img src="/2019/09/07/Docker创建镜像/1.png" title="avatar"><p>FROM 指定使用的镜像源; RUN  告诉docker在镜像中执行的命令。<br>要成功实现该示例，这里还差两个文件：requirement.txt 和 app.py。  </p><img src="/2019/09/07/Docker创建镜像/2.jpg" title="avatar"><img src="/2019/09/07/Docker创建镜像/3.jpg" title="avatar">  </li><li><p>准备工作完成之后，可以使用docker build命令构建镜像；  </p><blockquote><p>docker   build   –tag=ImageName   .  </p><p>说明：  </p><ul><li>命令最后一个”.”，用来指定创建镜像的资源目录，即Dockerfile的目录。  </li><li>-t指定创建的镜像名。</li></ul></blockquote></li><li><p>docker images 查看创建的镜像信息；</p></li><li><p>启动容器；</p><blockquote><p>docker run -p 4000:80 ImageName</p></blockquote><img src="/2019/09/07/Docker创建镜像/4.jpg" title="avatar"></li><li><p>停止容器运行；</p><blockquote><p> docker container stop Conatiner_ID</p></blockquote></li><li><p>使用docker tag命令为镜像添加一个新的标签。</p><blockquote><p> docker tag Container_ID  ImageName:TagName</p></blockquote></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker命令</title>
      <link href="/2019/09/07/Docker%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/09/07/Docker%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker常用命令"><a href="#Docker常用命令" class="headerlink" title="Docker常用命令"></a>Docker常用命令</h1><ol><li>docker run hello-world 启动hello-world容器镜像<br> docker run -t -i ubuntu:18.04 /bin/bash   运行交互式容器<br> -t : 在新容器内指定一个伪终端或者终端<br>  -i : 通过容器内的标准输入STDIN进行交互<br> docker run -d hello-world  运行容器的后台模式<br> -d : 分离模式，在后台运行</li></ol><ol start="2"><li><p>docker –version或者docker version  查看版本信息</p></li><li><p>docker images或者docker image ls   罗列下载的容器镜像信息</p></li><li><p>docker container ls  查看正在运行的容器实例<br> docker container ls -all  查看所有容器实例</p></li><li><p>docker logs Container_ID   查看容器的标准输出</p></li><li><p>docker ps  查看正在运行的容器实例</p></li><li><p>docker stop Container_ID 或者 Ctrl+D 停止容器的运行</p></li><li><p>docker pull Image_name    载入镜像</p></li><li><p>docker port Container_ID   查看指定容器的端口映射<br> -p 主机端口/容器端口   将容器端口映射到主机端口</p></li><li><p>docker top Container_ID   查看容器内部运行的进程</p></li><li><p>docker rm 移除不需要的Docker镜像</p></li><li><p>docker search Name  搜索镜像</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Push Docker镜像到远程仓库</title>
      <link href="/2019/09/06/Push-Docker%E9%95%9C%E5%83%8F%E5%88%B0%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93/"/>
      <url>/2019/09/06/Push-Docker%E9%95%9C%E5%83%8F%E5%88%B0%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<p>将本地Docker镜像Push到远程仓库，命令如下：  </p><blockquote><p>docker tag local-image:tagname new-repo:tagname<br>docker push new-repo:tagname</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac安装Docker</title>
      <link href="/2019/09/06/Mac%E5%AE%89%E8%A3%85Docker/"/>
      <url>/2019/09/06/Mac%E5%AE%89%E8%A3%85Docker/</url>
      
        <content type="html"><![CDATA[<p>Mac上安装Docker比较简单，直接从<a href="https://hub.docker.com/editions/community/docker-ce-desktop-mac" target="_blank" rel="noopener">官网</a>下载安装文件，再安装就可以了。  </p><p>查看是否安装成功：</p><blockquote><p>docker –version</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu安装Docker</title>
      <link href="/2019/09/04/Ubuntu%E5%AE%89%E8%A3%85Docker/"/>
      <url>/2019/09/04/Ubuntu%E5%AE%89%E8%A3%85Docker/</url>
      
        <content type="html"><![CDATA[<ol><li>从Docker仓库下载安装，首先确保能够访问Docker仓库地址:<a href="https://download.docker.com/linux/ubuntu，" target="_blank" rel="noopener">https://download.docker.com/linux/ubuntu，</a> 如果能够访问，按照下面的操作步骤进行安装。  <blockquote><p>sudo apt update<br>sudo apt install apt-transport-https ca-certificates curl software-properties-common</p></blockquote></li><li>在/etc/apt/sources.list.d/docker.list文件中添加下面内容；  <blockquote><p>deb [arch=amd64] <a href="https://download.docker.com/linux/ubuntu" target="_blank" rel="noopener">https://download.docker.com/linux/ubuntu</a> bionic stable</p></blockquote></li><li>添加秘钥；  <blockquote><p>curl -fsSL <a href="https://download.docker.com/linux/ubuntu/gpg" target="_blank" rel="noopener">https://download.docker.com/linux/ubuntu/gpg</a> | sudo apt-key add -</p></blockquote></li><li>安装docker-ce；  <blockquote><p>sudo apt install docker-ce</p></blockquote></li><li>查看是否安装成功。  <blockquote><p>docker –version<br>输出结果： Docker version 19.03.2, build 6a30dfc</p></blockquote></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VSCode远程连接开发</title>
      <link href="/2019/09/02/VSCode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E5%BC%80%E5%8F%91/"/>
      <url>/2019/09/02/VSCode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<p>说明： 示例是Mac上的VSCode连接远程Ubuntu服务器。</p><h2 id="一、-服务器配置"><a href="#一、-服务器配置" class="headerlink" title="一、 服务器配置"></a>一、 服务器配置</h2><ol><li>首先，给Ubuntu安装openssh-server；  <blockquote><p>sudo apt-get install openssh-server</p></blockquote></li><li>其次，启动ssh；  <blockquote><p>sudo service ssh start</p></blockquote></li><li>检查ssh是否开启。<blockquote><p>ps -e | grep ssh</p></blockquote></li></ol><h2 id="二、-本地配置"><a href="#二、-本地配置" class="headerlink" title="二、 本地配置"></a>二、 本地配置</h2><ol><li>Mac本身自带了ssh，不必安装；  </li><li>查看是否存在ssh密钥: ~/.ssh/id_rsa_pub；  </li><li>不存在的话，输入命令： ssh-keygen -t rsa -b 4096 ，生成两个文件；  <img src="/2019/09/02/VSCode远程连接开发/key.jpg" title="avatar"></li><li>将本地的公共密钥发送到服务器。  <blockquote><p>ssh-copy-id <a href="mailto:wangmeng@10.61.3.212" target="_blank" rel="noopener">wangmeng@10.61.3.212</a>  </p></blockquote></li></ol><h2 id="三、-VSCode配置"><a href="#三、-VSCode配置" class="headerlink" title="三、 VSCode配置"></a>三、 VSCode配置</h2><ol><li>首先安装VSCode Remote SSH，安装完左侧会显示新图标；  </li><li>点击CONNECTIONS后面的设置图标，再点击第一项进行配置； <img src="/2019/09/02/VSCode远程连接开发/1.jpg" title="avatar"></li><li>设置连接名、服务器地址、服务器上的用户名。  <img src="/2019/09/02/VSCode远程连接开发/2.jpg" title="avatar"><img src="/2019/09/02/VSCode远程连接开发/3.jpg" title="avatar"></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> Mac </tag>
            
            <tag> VSCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用conda安装Python模块</title>
      <link href="/2019/09/02/%E4%BD%BF%E7%94%A8conda%E5%AE%89%E8%A3%85Python%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/09/02/%E4%BD%BF%E7%94%A8conda%E5%AE%89%E8%A3%85Python%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<p>问题：在终端中pip成功安装的模块，在<em>Jupyter-Notebook</em>中<em>import</em>失败，原因在于终端和Jupyter-Notebook使用的Python内核位置不同。  </p><p>如何解决：在Anaconda的bin目录下，使用pip重新安装，conda list查看是否安装成功。  </p><img src="/2019/09/02/使用conda安装Python模块/eg.jpg" title="avatar">]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python中join问题</title>
      <link href="/2019/09/02/Python%E4%B8%ADjoin%E9%97%AE%E9%A2%98/"/>
      <url>/2019/09/02/Python%E4%B8%ADjoin%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>以<em>‘ ‘.join()</em>的方式将列表转为字符串，要求list中的元素都是字符串，否则需要<em>map(str, list)</em>进行转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = [&#123;<span class="string">"a"</span>:<span class="number">1</span>&#125;, &#123;<span class="string">"b"</span>:<span class="number">2</span>&#125;]  </span><br><span class="line"><span class="string">','</span>.join(map(str, a))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu SSR</title>
      <link href="/2019/09/01/Ubuntu-SSR/"/>
      <url>/2019/09/01/Ubuntu-SSR/</url>
      
        <content type="html"><![CDATA[<h2 id="Ubuntu安装ssr客户端"><a href="#Ubuntu安装ssr客户端" class="headerlink" title="Ubuntu安装ssr客户端"></a>Ubuntu安装ssr客户端</h2><blockquote><ul><li>wget <a href="http://www.djangoz.com/ssr" target="_blank" rel="noopener">http://www.djangoz.com/ssr</a>  </li><li>sudo mv ssr /usr/local/bin  </li><li>sudo chmod 766 /usr/local/bin/ssr  </li><li>ssr install  </li><li>ssr config  </li></ul></blockquote><p>最后一步ssr config，将找的有效节点信息的配置导入，比如用下图中的内容替换config中的内容。<br>有效节点可以在youtube吾爱分享频道、yassuo.xyz、woocloud.online等等的站点获取。  </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"server"</span>: <span class="string">"jp1.ssrnode.top"</span>,</span><br><span class="line">    <span class="attr">"local_address"</span>: <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="attr">"local_port"</span>: <span class="number">1080</span>,</span><br><span class="line">    <span class="attr">"timeout"</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="attr">"workers"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"server_port"</span>: <span class="number">61536</span>,</span><br><span class="line">    <span class="attr">"password"</span>: <span class="string">"8ELIWb"</span>,</span><br><span class="line">    <span class="attr">"method"</span>: <span class="string">"rc4-md5"</span>,</span><br><span class="line">    <span class="attr">"obfs"</span>: <span class="string">"http_simple"</span>,</span><br><span class="line">    <span class="attr">"obfs_param"</span>: <span class="string">"3027117564.microsoft.com"</span>,</span><br><span class="line">    <span class="attr">"protocol"</span>: <span class="string">"auth_aes128_sha1"</span>,</span><br><span class="line">    <span class="attr">"protocol_param"</span>: <span class="string">"17564:7vdNqp"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果没有git，需要安装： <em>sudo apt-get install git</em></p><h2 id="Ubuntu代理设置"><a href="#Ubuntu代理设置" class="headerlink" title="Ubuntu代理设置"></a>Ubuntu代理设置</h2><p>Ubuntu与Mac和Windows不同，浏览器需要手动设置代理。  </p><ol><li>首先安装SwitchyOmega插件，如果安装Google拒绝安装离线插件，可以先将crx文件改成zip或者rar文件，再加载已解压文件，可以成功安装。  </li><li>安装成功后，再进行配置。新建情景模式或者直接在默认的proxy中按图所示进行修改，主要是代理协议、服务器和代理端口。<img src="/2019/09/01/Ubuntu-SSR/proxy.jpg" title="avatar"></li><li>可以安装一些规则，填完规则列表格式和网址之后，点击立即更新情景模式。  <img src="/2019/09/01/Ubuntu-SSR/principle.jpg" title="avatar"></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac配置Tunnelblick</title>
      <link href="/2019/09/01/Mac%E9%85%8D%E7%BD%AETunnelblick/"/>
      <url>/2019/09/01/Mac%E9%85%8D%E7%BD%AETunnelblick/</url>
      
        <content type="html"><![CDATA[<ol><li>Tunnelblick破解版安装;</li><li>安装之后，点击Tunnelblick，选择我没有配置文件，为我生成配置文件;</li><li>打开自动生成的文件夹，打开config.ovpn;</li><li>配置文件设置：将OpenVPN解压，获取config文件夹，将其中的client.ovpn中的内容复制到config.ovpn中，同时将config文件夹中的其他文件复制到自动生成的文件夹中;</li><li>将自动生成的文件夹改名为trec6.tblk;</li><li>点击右上角的tunnelblick图标，选择VPN详情，将trec6.tblk拖到配置栏中。或者双击进行配置。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 软件 </tag>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
